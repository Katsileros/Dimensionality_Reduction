\chapter{Εφαρμογή του αλγορίθμου \textlatin{LLE} και των δύο μεθόδων σε πραγματικά σετ δεδομένων}
\section{Στόχος των πειραμάτων}
\par
Στην εργασία αυτή δόθηκε έμφαση στην μελέτη του αλγορίθμου με συγκεκριμένα σετ δεδομένων και παράλληλα διερευνήθηκε σε μεγάλο βαθμό το πως επηρεάζουν την συμπεριφορά του οι παράμετροι που δέχεται ως είσοδο απο τον χρήστη. Αυτές είναι ο αριθμός των κοντινότερων γειτόνων \textlatin{(k)} για το πρώτο βήμα του αλγορίθμου και ο αριθμός των τελικών διστάσεων \textlatin{(d)} για το τελικό βήμα του. Επίσης, γίνεται σύγκριση στην απόδοση του αλγορίθμου ταξινόμησης κοντινότερων γειτόνων \textlatin{(k-NN)} για τον χώρο αρχικών διαστάσεων \textlatin{D} και αυτού των τελικών \textlatin{d}. Τελικός στόχος λοιπόν έπειτα απο την εκτέλεση των πειραμάτων είναι να καταλήξουμε στο συμπέρασμα κατά πόσο η μείωση των διαστάσεων με χρήση του αλγορίθμου \textlatin{LLE} μπορεί να συμβάλει θετικά στην βελτίωση του ποσοστού ταξινόμησης σε εφαρμογές Μηχανιής
Μάθησης.

\section{Πειράματα και Αποτελέσματα}

\subsection{Πειράματα - \textlatin{MNIST}}
\par
\href{http://yann.lecun.com/exdb/mnist/}{\textbf{\textlatin{MNIST: }}}Το πρώτο σετ δεδομένων το οποίο χρησιμοποιήθηκε είναι το πολύ γνωστό και ευρέως χρησιμοποιούμενο σετ δεδομένων στον χώρο της αναγνώρισης προτύπων, \textlatin{MNIST}\textlatin{\cite{mnist}}. Το σετ αυτό αποτελείται απο 70.000 εικόνες, διάστασης $[28 \times 28]$ \textlatin{pixel}, οι οποίες περιέχουν χειρόγραφα ψηφία. Οι 60.000 απο αυτές ανήκουν στο σετ εκπαίδευσης και οι 10.000 στο σετ αξιολόγησης. Για την είσοδο των δεδομένων στον αλγόριθμο, εφαρμόστηκε η λεξικογραφική διάταξη σε κάθε μια απο τις εικόνες, καταλήγοντας σε ένα διάνυσμα διάστασης $D = 784$. Τέλος, να διευκρινιστεί ότι το τελικό αποτέλεσμα ταξινόμησης, για τα σετ δεδομένων με τα ψηφία είναι το μέσο σφάλμα ταξινόμησης δηλαδή το άθροισμα του σφάλματος κάθε κλάσης προς τον συνολικό αριθμό των κλάσεων. Η επιλογή αυτή έγινε, διότι η μετρική αυτή δίνει έναν πολύ πιο ακριβές και γενικευμένο αποτέλεσμα ως προς την απόδοση του αλγορίθμου.

\par
Στο πρώτο πείραμα με αυτό το σετ δεδομένων διερευνήθηκε αρχικά η συμπεριφορά του αλγορίθμου ως προς την απόδοση του αποτελέσματος ταξινόμησης μετά την μείωση των διαστάσεων. Οι παράμετροι οι οποίες αξιολογήθηκαν είναι ο αριθμός \textlatin{K} των κοντινότερων γειτόνων, ο αριθμός των τελικών διαστάσεων \textlatin{d} αλλά και ο αριθμός των υποσυνόλων της Μεθόδου-2. Συγκεκριμένα για τον αριθμό κοντινότερων γειτόνων του πρώτου βήματος του αλγορίθμου δόθηκαν οι τιμές $K = 6,7,8,9,10,12,16,20,24,32,64$, για τον αριθμό των τελικών διαστάσεων του τελευταίου βήματος οι τιμές $d = 10,16,20,24,32,40,52,64,96,128,256$ και για τον αριθμό των δειγμάτων των υποσυνόλων οι τιμές $batch\textunderscore size = 10.000,20.000,60.000$ .Δηλαδή χωρίσαμε το σετ δεδομένων εκπαίδευσης των 60.000 εικόνων σε 6,3,1 υποσύνολα αντίστοιχα. Για το πείραμα αυτό δεν χρησιμοποιήσαμε την Μέθοδο-1, δηλαδή σε κάθε υποσύνολο κάθε φορά ενσωματώθηκαν τα δεδομένα αξιολόγησης στα δεδομένα εκπαίδευσης και στην συνέχεια έγινε εφαρμογή του αλγόριθμου για την μείωση των διαστάσεων στο σύνολο των δεδομένων αυτών. Για την εξαγωγή του τελικού αποτελέσματος εφαρμόστηκε ο αλγόριθμος \textlatin{k-NN} με $k=2$. Το μικρότερο ποσοστό σφάλματος ταξινόμησης δόθηκε για τις παραμέτρους $\mathbf{K=12, d=128, batch\textunderscore size=60.000}$, με τιμή \textbf{3.06\%} έναντι του \textbf{3.5\%} το οποίο είναι το σφάλμα ταξινόμησης στον χώρο των αρχικών διαστάσεων \textlatin{D}. Πολύ μεγάλο ενδιαφέρον παρουσιάζει το γεγονός ότι το σφάλμα για τις παραμέτρους $\mathbf{K=8, d=10, batch\textunderscore size=60.000}$ ισούτε με \textbf{3.31\%} το οποίο είναι και αυτό μικρότερο απο την ταξινόμηση πρίν την μείωση διαστάσεων. Αξίζει να δοθεί έμφαση στην συγκεκριμένη αυτή περίπτωση διότι έχουμε καλύτερο ποσοστό ταξινόμησης έπειτα απο δραματική μείωση των διαστάσεων, αφού απο τις 784 αρχικές επιλέγουμε τελικά να κρατήσουμε 10 τελικές. 
\par
Παρατηρώντας τα αποτελέσματα των παρακάτω πινάκων μπορεί να παρατηρήσει κανείς ότι για την περίπτωση όπου έχουμε 64 ή περισσότερες τελικές διαστάσεις το τελικό ποσοστό σφάλματος είναι μικρότερο ή οριακά ίσο με αυτό του χώρου των αρχικών διαστάσεων. Το ίδιο ισχύει και για την περίπτωση στην οποία έχουμε $batch\textunderscore size=20.000$. Το γεγονός αυτό μας δίνει ένα πολύ μεγάλο πλεονέκτημα διότι χωρίζοντας το σετ δεδομένων εκπαίδευσης των 60.000 δειγμάτων, σε 3 υποσύνολα έχουμε τεράστια μείωση στο κόστος των υπολογισμών αλλά και στην διαθέσιμη μνήμη η οποία απαιτείται για την εκτέλεση του αλγορίθμου. Ακόμα πιο ενδιαφέρον είναι το αποτέλεσμα του πειράματος με παραμέτρους $\mathbf{K=10, d=128, batch\textunderscore size=10.000}$ για το οποίο έχουμε σφάλμα ταξινόμησης \textbf{3.31\%}, αποτέλεσμα μικρότερο απο αυτό των αρχικών διαστάσεων και μάλιστα πολύ οικονομικότερο στον χρόνο υπολογισμού διότι στην περίπτωση αυτή έχουμε χωρίσει το σετ εκπαίδευσης σε \textbf{6} μικρούς σχετικά υποχώρους οι οποίοι μειώνουν την πολυπλοκότητα επίλυσης του προβλήματος κατά πολλές τάξεις μεγέθους. Οι παρακάτω πίνακες δείχνουν τα αποτέλεσμα του σφάλματος ταξινόμησης για όλους τους συνδυασμούς των παραμέτρων, και φανερώνουν σημαντικά στοιχεία για την αποτελεσματικότητα της μείωσης των διαστάσεων. Με έντονη γραμματοσειρά είναι ποσοστά σφάλματος μικρότερα απο το \textbf{3.5\%} των αρχικών \textlatin{D} διαστάσεων.

\newpage{}
\selectlanguage{english}
\begin{table}[H]
\centering
\singlespacing
\label{tab:table1}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (Method-2: subspaces=6)}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Μέθοδος-2: 6 υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
& K=6 & K=7 & K=8 & K=9 & K=10 & K=12 & K=16 & K=20 & K=24 & K=32 & K=64 \\
\hline
d=10 & 4.07 & 4.18 & 4.20 & 4.48 & 4.60 & 4.69 & 5.82 & 7.35 & 9.87 & 13.2 & 22.83 \\
d=16 & 3.91 & 3.98 & 4.06 & 4.22 & 4.20 & 4.36 & 5.29 & 5.92 & 7.81 & 10.45 & 17.10 \\
d=20 & 4.03 & 3.99 & 4.16 & 4.15 & 4.14 & 4.44 & 4.96 & 5.69 & 6.95 & 9.31 & 15.55 \\
d=24 & 4.13 & 4.05 & 4.10 & 4.14 & 4.22 & 4.13 & 4.77 & 5.40 & 6.35 & 8.32 & 14.12 \\
d=32 & 3.92 & 3.91 & 4.03 & 3.98 & 4.07 & 4.18 & 4.27 & 5.07 & 5.78 & 7.37 & 12.36 \\
d=40 & 4.00 & 3.82 & 3.90 & 3.99 & 3.97 & 4.03 & 4.15 & 4.72 & 5.22 & 6.41 & 11.93 \\
d=52 & 3.93 & 3.78 & 3.97 & 3.83 & 3.93 & 3.97 & 4.17 & 4.37 & 4.89 & 6.30 & 10.74 \\
d=64 & 3.95 & 3.76 & 3.77 & 3.87 & 3.80 & 3.87 & 3.81 & 4.14 & 4.61 & 6.05 & 10.28 \\
d=96 & 4.02 & \underline{3.6} & \underline{3.7} & \underline{3.68} & \underline{3.68} & \underline{3.72} & \underline{3.63} & 3.90 & 4.22 & 5.51 & 10.06 \\
d=128 & 3.89 & \underline{3.67} & \underline{3.59} & \textbf{\underline{3.51}} & \textbf{\underline{3.31}} & \underline{3.66} & \underline{3.60} & 3.98 & 4.43 & 5.29 & 10.01 \\
d=256 & 3.77 & \underline{3.64} & \textbf{\underline{3.51}} & \textbf{\underline{3.33}} & \textbf{\underline{3.47}} & \textbf{\underline{3.34}} & \underline{3.56} & 4.06 & 4.61 & 5.40 & 9.43 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table2}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (Method-2: subspaces=3)}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Μέθοδος-2: 3 υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
& K=6 & K=7 & K=8 & K=9 & K=10 & K=12 & K=16 & K=20 & K=24 & K=32 & K=64 \\
\hline
d=10 & 3.90 & 3.75 & 3.92 & 4.07 & 3.97 & 4.60 & 5.49 & 6.64 & 10.66 & 14.99 & 31.58 \\
d=16 & 3.88 & 3.80 & 3.75 & 4.00 & 3.87 & 4.19 & 4.86 & 5.63 & 7.55 & 10.45 & 20.65 \\
d=20 & 3.81 & 3.82 & 3.84 & 3.89 & 3.95 & 4.01 & 4.61 & 5.40 & 6.92 & 9.88 & 17.65 \\
d=24 & \underline{3.73} & \underline{3.73} & 3.81 & 3.87 & 3.87 & 3.90 & 4.47 & 5.29 & 6.19 & 9.28 & 15.88 \\
d=32 & \underline{3.78} & \underline{3.63} & \underline{3.72} & \underline{3.65} & 3.82 & 3.79 & 4.26 & 4.94 & 5.53 & 8.23 & 13.69 \\
d=40 & \underline{3.73} & \underline{3.70} & \underline{3.73} & \underline{3.71} & \underline{3.71} & \underline{3.75} & 4.09 & 4.51 & 5.57 & 6.90 & 12.43 \\
d=52 & \underline{3.77} & \underline{3.73} & \underline{3.61} & \underline{3.66} & \underline{3.61} & \underline{3.68} & 4.05 & 4.30 & 4.85 & 6.36 & 11.09 \\
d=64 & \underline{3.73} & \underline{3.65} & \underline{3.66} & \underline{3.57} & \underline{\textbf{3.52}} & \underline{3.76} & 3.81 & 4.21 & 4.79 & 5.91 & 10.56 \\
d=96 & \underline{3.65} & \underline{3.64} & \underline{\textbf{3.51}} & \underline{3.56} & \underline{\textbf{3.49}} & \underline{\textbf{3.41}} & \underline{3.63} & 3.92 & 4.23 & 5.25 & 10.02 \\
d=128 & 3.81 & \underline{\textbf{3.53}} & \underline{\textbf{3.52}} & \underline{\textbf{3.48}} & \underline{\textbf{3.47}} & \underline{\textbf{3.35}} & \underline{3.71} & 3.88 & 4.22 & 5.21 & 9.48 \\
d=256 & \underline{3.57} & \underline{\textbf{3.46}} & \underline{\textbf{3.39}} & \underline{\textbf{3.30}} & \underline{\textbf{3.25}} & \underline{\textbf{3.37}} & \underline{\textbf{3.27}} & \underline{3.66} & 4.32 & 5.47 & 8.73 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\singlespacing
\label{tab:table3}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (No subspaces)}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Χωρίς υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
& K=6 & K=7 & K=8 & K=9 & K=10 & K=12 & K=16 & K=20 & K=24 & K=32 & K=64 \\
\hline
d=10 & \underline{\textbf{3.56}} & \underline{\textbf{3.35}} & \underline{\textbf{3.31}} & \underline{\textbf{3.53}} & \underline{3.63} & 4.10 & 4.48 & 5.59 & 9.41 & 16.72 & 37.48 \\
d=16 & \underline{\textbf{3.48}} & \underline{\textbf{3.35}} & \underline{\textbf{3.44}} & \underline{\textbf{3.42}} & \underline{\textbf{3.40}} & \underline{3.67} & 4.10 & 5.00 & 7.07 & 11.39 & 25.14 \\
d=20 & \underline{\textbf{3.41}} & \underline{\textbf{3.31}} & \underline{\textbf{3.40}} & \underline{\textbf{3.40}} & \underline{\textbf{3.41}} & \underline{\textbf{3.53}} & 4.08 & 4.56 & 6.57 & 9.63 & 22.04 \\
d=32 & \underline{\textbf{3.45}} & \underline{\textbf{3.39}} & \underline{\textbf{3.42}} & \underline{\textbf{3.51}} & \underline{\textbf{3.35}} & \underline{\textbf{3.30}} & 3.90 & 4.55 & 6.18 & 9.35 & 19.53 \\
d=24 & \underline{3.62} & \underline{\textbf{3.33}} & \underline{\textbf{3.47}} & \underline{\textbf{3.45}} & \underline{\textbf{3.26}} & \underline{\textbf{3.40}} & \underline{\textbf{3.51}} & 4.24 & 5.36 & 8.45 & 16.59 \\
d=40 & \underline{\textbf{3.52}} & \underline{\textbf{3.34}} & \underline{\textbf{3.54}} & \underline{\textbf{3.46}} & \underline{\textbf{3.37}} & \underline{\textbf{3.37}} & \underline{\textbf{3.48}} & 4.18 & 5.14 & 7.91 & 14.58 \\
d=52 & \underline{\textbf{3.43}} & \underline{\textbf{3.13}} & \underline{\textbf{3.41}} & \underline{\textbf{3.34}} & \underline{\textbf{3.35}} & \underline{\textbf{3.44}} & \underline{\textbf{3.44}} & \underline{\textbf{3.64}} & 4.96 & 7.15 & 12.27 \\
d=64 & \underline{\textbf{3.52}} & \underline{\textbf{3.20}} & \underline{\textbf{3.36}} & \underline{\textbf{3.35}} & \underline{\textbf{3.34}} & \underline{\textbf{3.43}} & \underline{\textbf{3.46}} & \underline{3.68} & 4.89 & 6.52 & 11.06 \\
d=96 & \underline{\textbf{3.24}} & \underline{\textbf{3.10}} & \underline{\textbf{3.26}} & \underline{\textbf{3.21}} & \underline{\textbf{3.22}} & \underline{\textbf{3.33}} & \underline{\textbf{3.35}} & \underline{3.61} & 4.34 & 6.05 & 10.37 \\
d=128 & \underline{\textbf{3.19}} & \underline{\textbf{3.25}} & \underline{\textbf{3.11}} & \underline{\textbf{3.30}} & \underline{\textbf{3.12}} & \underline{\textbf{3.06}} & \underline{\textbf{3.34}} & \underline{\textbf{3.55}} & 4.06 & 5.84 & 10.18 \\
d=256 & \underline{\textbf{3.18}} & \underline{\textbf{3.34}} & \underline{\textbf{3.22}} & \underline{\textbf{3.21}} & \underline{\textbf{3.18}} & \underline{\textbf{3.14}} & \underline{\textbf{3.17}} & \underline{3.62} & 3.88 & 5.72 & 9.51 \\
\hline
\end{tabular}
\end{table}

\selectlanguage{greek}
\par
Απο την στιγμή που απο τα παραπάνω αποτελέσματα επιβεβαιώθηκε το γεγονός ότι μπορούμε να πάρουμε καλύτερο αποτέλεσμα ταξινόμησης εφαρμόζοντας την μέθοδο της διάσπασης του σετ εκπαίδευσης σε υποσύνολα, και μάλιστα με την διαδικασία της ταξινόμησης να είναι πολυ οικονομικότερη αλλά και γρηγορότερη, εστιάσαμε στον τρόπο με τον οποίο γίνεται η επιλογή των δεδομένων με σκοπό την δημιουργία των τελικών υποσυνόλων. Σκεφτήκαμε λοιπόν την περίπτωση για την οποία θα μπορούσε να δημιουργηθεί ένας υποχώρος, ο οποίος να περιέχει την χρήσιμη πληροφορία απο ολόκληρο το σετ εκπαίδευσης. Δηλαδή, σύμφωνα με την παραπάνω μέθοδο απο όλα τα τελικά υποσύνολα. Ο τρόπος με τον οποίο προσπαθήσαμε να οδηγηθούμε σε αυτό το αποτέλεσμα είναι η εφαρμογή αλγορίθμων ομαδοποίησης των δεδομένων. Συγκεκριμένα εφαρμόστηκε ο αλγόριθμος \href{https://en.wikipedia.org/wiki/K-means_clustering}{\textlatin{K-means}}\textlatin{\cite{kmeans}} επιλέγοντας σαν τελικά αντιπροσωπευτικά σημεία για το τελικό σύνολο δεδομένων εκπαίδευσης, το αποτέλεσμα του αλγορίθμου το οποίο είναι τα κεντροειδή σημεία τα οποία αντιπροσωπεύουν τις επιμέρους ομάδες. Δοκιμάστηκαν διαφορετικές τιμές για το σύνολο των τελικών κεντροειδών, καταλήγοντας στο βέλτιστο αποτέλεσμα για την περίπτωση στην οποία έχουμε $\mathbf{K=9, d=128, clustSize=20.000}$. Το σφάλμα της ταξινόμησης για την περίπτωση αυτή είναι \textbf{3.28\%}, μικρότερο απο αυτό των αρχικών διαστάσεων \textbf{(3.5\%)}. Συγκεκριμένες τιμές για τις παραμέτρους είναι $\mathbf{K=8,9,10,12,16,20}$, \textlatin{d} όπως και στο προηγούμενο ερώτημα και $\mathbf{clustSize=5.000, 10.000, 15.000, 20.000}$. Το σύνολο των αποτελεσμάτων φαίνεται στους παρακάτω πίνακες

\selectlanguage{english}
\begin{table}[H]
\singlespacing
\centering
\label{tab:table4}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων και δειγματοληψία με τον αλγόριθμο \textlatin{K-means} με 5.000 κεντροειδή.}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 5.91 & 6.75 & 7.28 & 8.78 & 10.35 & 16.21 \\
d=16 & 5.31 & 6.15 & 6.44 & 7.54 & 9.25 & 11.56 \\
d=20 & 5.16 & 5.25 & 5.31 & 6.37 & 8.66 & 10.85 \\
d=24 & 5.11 & 5.08 & 5.13 & 6.09 & 7.97 & 10.13 \\
d=32 & 5.01 & 5.10 & 5.18 & 5.86 & 6.75 & 9.18 \\
d=40 & 4.90 & 5.00 & 4.81 & 5.35 & 6.50 & 8.32 \\
d=52 & 5.08 & 5.02 & 5.00 & 5.53 & 6.48 & 7.51 \\
d=64 & 4.90 & 4.99 & 5.02 & 5.61 & 6.08 & 6.97 \\
d=96 & 4.76 & 4.82 & 5.05 & 5.29 & 5.87 & 6.42 \\
d=128 & 4.79 & 4.96 & 5.02 & 5.22 & 5.70 & 5.77 \\
d=164 & 4.88 & 4.98 & 4.97 & 5.38 & 5.41 & 5.47 \\
d=196 & 5.08 & 5.09 & 5.11 & 5.42 & 5.39 & 5.32 \\
d=256 & 5.33 & 5.33 & 5.30 & 5.17 & 5.45 & 5.18 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (K-means subsampling - 5.000 centroids)}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table5}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων και δειγματοληψία με τον αλγόριθμο \textlatin{K-means} με 10.000 κεντροειδή.}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 4.98 & 5.87 & 6.18 & 7.32 & 9.04 & 13.17 \\
d=16 & 4.69 & 4.89 & 5.03 & 6.49 & 7.66 & 11.22 \\
d=20 & 4.37 & 4.70 & 4.83 & 5.88 & 6.82 & 9.74 \\
d=24 & 4.33 & 4.62 & 4.67 & 5.49 & 6.50 & 8.30 \\
d=32 & 4.38 & 4.53 & 4.72 & 5.20 & 6.08 & 7.55 \\
d=40 & 4.35 & 4.35 & 4.87 & 5.13 & 5.76 & 6.70 \\
d=52 & 4.31 & 4.18 & 4.52 & 5.26 & 5.78 & 6.61 \\
d=64 & 4.31 & 4.42 & 4.60 & 5.06 & 5.43 & 6.44 \\
d=96 & 4.13 & 4.23 & 4.30 & 4.88 & 5.14 & 5.67 \\
d=128 & 4.07 & 4.06 & 4.32 & 4.68 & 5.01 & 5.64 \\
d=164 & 3.98 & 4.13 & 4.37 & 4.67 & 5.08 & 5.09 \\
d=196 & 4.19 & 4.38 & 4.39 & 4.65 & 4.85 & 5.17 \\
d=256 & 4.37 & 4.48 & 4.54 & 4.65 & 4.90 & 4.82 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (K-means subsampling - 10.000 centroids)}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table6}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων και δειγματοληψία με τον αλγόριθμο \textlatin{K-means} με 15.000 κεντροειδή.}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 4.61 & 4.86 & 5.54 & 6.35 & 8.23 & 12.80 \\
d=16 & 4.09 & 4.27 & 4.72 & 5.18 & 7.38 & 9.52 \\
d=20 & 3.94 & 4.21 & 4.47 & 5.12 & 6.91 & 8.38 \\
d=24 & 3.87 & 4.05 & 4.39 & 4.86 & 6.48 & 7.91 \\
d=32 & 4.05 & 4.07 & 4.22 & 4.65 & 5.32 & 7.49 \\
d=40 & \underline{3.72} & 3.88 & 4.11 & 4.43 & 4.94 & 6.75 \\
d=52 & 3.78 & 3.97 & 4.00 & 4.30 & 4.74 & 6.2 \\
d=64 & 3.79 & 3.94 & 4.07 & 4.38 & 4.73 & 5.81 \\
d=96 & \underline{3.76} & 3.81 & 4.14 & 4.50 & 4.64 & 5.45 \\
d=128 & \underline{3.74} & 3.96 & 4.25 & 4.34 & 4.67 & 5.08 \\
d=164 & 3.96 & 4.18 & 4.08 & 4.39 & 4.76 & 4.69 \\
d=196 & 4.03 & 4.18 & 4.04 & 4.30 & 4.73 & 4.72 \\
d=256 & 3.98 & 4.18 & 4.04 & 4.42 & 4.93 & 4.52 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (K-means subsampling - 15.000 centroids)}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table7}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων και δειγματοληψία με τον αλγόριθμο \textlatin{K-means} με 20.000 κεντροειδή.}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 4.29 & 4.52 & 4.83 & 5.65 & 6.97 & 10.78 \\
d=16 & 3.82 & 4.10 & 4.18 & 4.99 & 6.01 & 7.69 \\
d=20 & 3.80 & 4.00 & 4.02 & 4.70 & 5.59 & 7.58 \\
d=24 & 3.82 & 3.96 & 4.03 & 4.45 & 5.01 & 6.78 \\
d=32 & 3.91 & 3.92 & 4.14 & 4.34 & 4.62 & 6.02 \\
d=40 & \underline{3.74} & \underline{3.75} & 4.03 & 4.42 & 4.57 & 5.67 \\
d=52 & \underline{3.73} & \underline{3.64} & 3.88 & 4.23 & 4.54 & 5.21 \\
d=64 & \underline{3.69} & \underline{3.58} & 3.83 & 4.17 & 4.30 & 5.01 \\
d=96 & \underline{\textbf{3.54}} & \underline{3.60} & \underline{3.58} & 3.92 & 4.31 & 4.85 \\
d=128 & \underline{\textbf{3.42}} & \underline{\textbf{3.28}} & \underline{\textbf{3.53}} & 3.88 & 4.18 & 4.88 \\
d=164 & \underline{\textbf{3.44}} & \underline{\textbf{3.32}} & \underline{\textbf{3.41}} & 3.79 & 4.57 & 4.74 \\
d=196 & \underline{\textbf{3.31}} & \underline{\textbf{3.40}} & \underline{\textbf{3.55}} & \underline{3.74} & 4.39 & 4.62 \\
d=256 & \underline{3.60} & \underline{\textbf{3.47}} & \underline{3.58} & \underline{3.71} & 4.20 & 4.43 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (K-means subsampling - 20.000 centroids)}
\end{table}

\selectlanguage{greek}
\par
Στον παραπάνω πίνακα ενδιαφέρον παρουσιάζει το γεγονός ότι έπειτα απο την δειγματοληψία μέσω του αλγορίθμου \textlatin{Kmeans}\textlatin{\cite{kmeans}} τα ελάχιστα σφάλματα ταξινόμησης είναι σε κάθε περίπτωση πολύ κοντά μεταξύ τους. Αυτό δηλαδή σημαίνει ότι απο το αρχικό σετ δεδομένων των 60.000 εικόνων, κάναμε σημαντική μείωση του αριθμού των δεδομένων σε κάθε περίπτωση αλλά παρ'όλα αυτά είχαμε ελάχιστη εως και μηδενική απώλεια πληροφορίας. Η εξήγηση σε αυτό, δίνεται απο το γεγονός ότι το σετ δεδομένων \textlatin{MNIST}\textlatin{\cite{mnist}} είναι στην ουσία συνθετικό σετ. δηλαδή ένας σχετικά μικρός αριθμός του συνόλου των δεδομένων είναι μοναδικά και τα υπόλοιπα προκύπτουν απο ομογενείς μετασχηματισμούς ή παραμορφώσεις αυτών. Ακόμα, επιβεβαιώνει το γεγονός ότι ο αλγόριθμος \textlatin{LLE} δεν απαιτεί έναν μεγάλο αριθμό δειγμάτων εκπαίδευσης όπως για παράδειγμα τα Νευρωνικά δίκτυα, αλλά αρκεί ένας ομοιόμορφα δειγματοληπτημένος χώρος των αρχικών δεδομένων ο οποίος να διατηρεί το λείο της πολλαπλότητας.
\par
Στο επόμενο πείραμα με το συγκεκριμένο σετ εφαρμόστηκε η Μέθοδος-1, δηλαδή έγινε προβολή των δεδομένων αξιολόγησης στον χώρο μειωμένης διάστασης μέσω του πίνακα γειτνίασης στον χώρο των αρχικών διαστάσεων. Παρατηρώντας τους παρακάτω πίνακες βλέπουμε ότι το ελάχιστο σφάλμα ταξινόμησης στον χώρο των τελικών διαστάσεων ισούτε με \textbf{3.85\%} \textbf{(\textlatin{K}=8}, \textbf{\textlatin{d}=256}, \\ \textbf{\textlatin{batch\textunderscore size=60.000})}, μεγαλύτερο δηλαδή απο αυτό των αρχικών διαστάσεων \textbf{(3.5\%)} οπότε θα μπορούσαμε να καταλήξουμε στο συμπέρασμα ότι η διαδικασία της μεθόδου δεν μας βοηθά στην βελτίωση του αποτελέσματος. Παρ' όλα αυτά αποτελεί έναν πολύ πρακτικό και γρήγορο τρόπο, σε σχέση με την εφαρμογή του αλγορίθμου στο σύνολο των δειγμάτων εκπαίδευσης αλλά και αξιολόγησης, ώστε να καταφέρουμε να μειώσουμε τις διαστάσεις των τελευταίων. Αυτό είναι χρήσιμο σε περιπτώσεις στις οποίες μπορούμε νε ανεχθούμε την μικρή αυτή σχετικά διαφορά σφάλματος, και στις οποίες χρησιμοποιούμε τα δεδομένα αξιολόγησης σε πολλά επόμενα βήματα εκτελώντας υπολογιστικά απαιτητικούς αλγορίθμους. Το αποτέλεσμα εφαρμογής της μεθόδου για το σύνολο των παραμέτρων φαίνεται στους παρακάτω πίνακες

\selectlanguage{english}
\begin{table}[H]
\singlespacing
\centering
\label{tab:table8}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Χρήση της Μεθόδου-1 και της Μεθόδου-2 με 6 υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 15.63 & 15.88 & 16.84 & 18.19 & 21.49 & 21.99 \\
d=16 & 9.97 & 10.06 & 10.53 & 10.79 & 14.76 & 17.78 \\
d=20 & 8.14 & 8.29 & 9.04 & 9.86 & 11.68 & 14.75 \\
d=24 & 6.02 & 6.44 & 7.00 & 8.74 & 10.26 & 12.41 \\
d=32 & 5.99 & 6.09 & 6.40 & 6.81 & 7.95 & 9.98 \\
d=40 & 5.72 & 5.59 & 5.85 & 6.01 & 7.10 & 7.84 \\
d=52 & 5.60 & 5.52 & 5.54 & 5.81 & 5.99 & 7.34 \\
d=64 & 5.42 & 5.37 & 5.49 & 5.53 & 5.81 & 6.26 \\
d=96 & 5.41 & 5.35 & 5.52 & 5.58 & 5.53 & 5.57 \\
d=128 & 5.19 & 5.18 & 5.19 & 5.31 & 5.32 & 5.43 \\
d=164 & 5.12 & 5.16 & 5.19 & 5.17 & 5.17 & 5.24 \\
d=196 & 5.09 & 5.07 & 5.06 & 5.08 & 5.08 & 5.21 \\
d=256 & 4.98 & 5.01 & 5.03 & 5.15 & 5.07 & 5.01 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (Method-1 and Method-2 - 6 subspaces)}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table9}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Χρήση της Μεθόδου-1 και της Μεθόδου-2 με 3 υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 14.60 & 14.38 & 15.74 & 17.68 & 21.48 & 24.07 \\
d=16 & 8.88 & 9.33 & 9.67 & 9.53 & 14.00 & 17.01 \\
d=20 & 5.78 & 5.93 & 6.32 & 8.01 & 11.54 & 14.23 \\
d=24 & 5.39 & 5.37 & 5.50 & 6.40 & 9.03 & 11.07 \\
d=32 & 5.05 & 5.15 & 5.26 & 5.41 & 6.67 & 9.19 \\
d=40 & 4.97 & 5.03 & 5.04 & 5.24 & 5.62 & 7.14 \\
d=52 & 4.97 & 5.06 & 4.88 & 4.92 & 5.37 & 6.25 \\
d=64 & 4.96 & 5.02 & 4.73 & 4.84 & 5.13 & 5.46 \\
d=96 & 4.80 & 4.87 & 4.84 & 4.77 & 4.90 & 5.06 \\
d=128 & 4.63 & 4.61 & 4.75 & 4.74 & 4.81 & 4.81 \\
d=164 & 4.59 & 4.58 & 4.52 & 4.60 & 4.65 & 4.80 \\
d=196 & 4.54 & 4.54 & 4.55 & 4.55 & 4.55 & 4.75 \\
d=256 & 4.54 & 4.44 & 4.43 & 4.44 & 4.52 & 4.60 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (Method-1 and Method-2 - 3 subspaces)}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table10}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{MNIST} με τον αλγόριθμο κοντινότερων γειτόνων (Χρήση της Μεθόδου-1 χωρίς υποσύνολα)}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& K=8 & K=9 & K=10 & K=12 & K=16 & K=20 \\
\hline
d=10 & 17.92 & 17.50 & 16.00 & 16.10 & 21.93 & 34.84 \\
d=16 & 7.09 & 7.52 & 8.43 & 8.57 & 12.95 & 19.03 \\
d=20 & 4.85 & 4.92 & 5.46 & 6.86 & 9.29 & 15.82 \\
d=24 & 4.70 & 4.67 & 4.61 & 4.74 & 8.49 & 12.86 \\
d=32 & 4.59 & 4.64 & 4.57 & 4.79 & 6.43 & 10.54 \\
d=40 & 4.52 & 4.47 & 4.47 & 4.58 & 4.78 & 8.90 \\
d=52 & 4.36 & 4.32 & 4.41 & 4.40 & 5.16 & 6.82 \\
d=64 & 4.36 & 4.32 & 4.32 & 4.46 & 5.05 & 5.12 \\
d=96 & 4.10 & 4.09 & 4.13 & 4.22 & 4.52 & 5.14 \\
d=128 & 4.08 & 4.08 & 4.12 & 4.05 & 4.32 & 4.52 \\
d=164 & 4.07 & 4.05 & 4.09 & 4.00 & 4.27 & 4.48 \\
d=196 & 3.92 & 4.00 & 4.07 & 4.02 & 4.24 & 4.39 \\
d=256 & \underline{3.85} & 3.92 & 3.97 & 4.04 & 4.17 & 4.35 \\
\hline
\end{tabular}
%\caption{MNIST k-NN Classification Mean Average (\%) Error (Method-1 - No subspaces)}
\end{table}
\selectlanguage{greek}

\subsection{Πειράματα - \textlatin{SVHN}}
\par
\href{http://ufldl.stanford.edu/housenumbers/}{\textbf{\textlatin{SVHN: }}}Το δεύτερο σετ δεδομένων είναι το \href{http://ufldl.stanford.edu/housenumbers/}{\textlatin{The Street View House Numbers (SVHN) Dataset}}\cite{12} το οποίο περιέχει πραγματικές εικόνες απο αριθμούς σπιτιών, οι οποίες τραβήχτηκαν απο το αυτοκίνητο χαρτογράφησης της \textlatin{Google}. Συγκεκριμένα χρησιμοποιήθηκε το \textlatin{Format-2} στο οποίο οι αριθμοί με περισσότερα απο ένα ψηφία έχουν διασπαστεί σε ψηφία απο το 0 εως το 9. Το σετ αυτό, είναι όμοιο με το \textlatin{MNIST}\textlatin{\cite{mnist}} με την διαφορά ότι πρόκειται για πραγματικές εικόνες, το οποίο καθιστά την διαδικασία της αναγνώρισης κατά πολύ δυσκολότερη. Αυτο συμβαίνει λόγω φυσικών παραμορφώσεων, για παράδειγμα απο την αλλοίωση της φωτεινότητας, την παρουσία θορύβου κλπ. Το σετ περιέχει 73.257 εικόνες στο σετ εκπαίδευσης και 26032 εικόνες στο σετ αξιολόγησης, μεγέθους $[32 \times 32]$ \textlatin{pixel}. Εφαρμόζοντας αντίστοιχα λεξικογραφική διάταξη στο σετ αυτό, καταλήγουμε να έχουμε για κάθε εικόνα ένα διάνυσμα μεγέθους $D = 1024$. 

\par
Αν για το συγκεκριμένο σετ δεδομένων εφαρμόσουμε την διαδικασία την οποία εφαρμόσαμε στο σετ δεδομένων \textlatin{MNIST}\textlatin{\cite{mnist}} θα παρατηρήσουμε ότι τα αποτελέσματά μας έχουν τεράστιο σφάλμα ταξινόμησης. Αυτό συμβαίνει διότι, υπάρχει πολύ μεγάλο πρόβλημα απο το πρώτο κιόλας βήμα του αλγορίθμου στο οποίο εφαρμόζεται ο αλγόριθμος \textlatin{k-NN} μεταξύ των δεδομένων αξιολόγησης και των δεδομένων εκπαίδευσης στον χώρο των αρχικών διαστάσεων \textlatin{D}. Το γεγονός αυτό προκύπτει απο την φύση των δεδομένων του σετ αυτού, και πιο συγκεκριμένα ο αλγόριθμος κοντινότερων γειτόνων αποτυγχάνει εξαιτίας του γεγονότος ότι πρόκειται για εικόνες απο το φυσικό περιβάλλον οι οποίες πάσχουν απο θόρυβο αλλά και παραμορφώσεις της φωτεινότητας. 
\par
\href{https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients}{\textbf{\textlatin{Histogram of Oriented Gradients (HoG) }}}\textlatin{\cite{hog}}: Για την αντιμετώπιση της παραπάνω αδυναμίας εκτέλεσης του αλγορίθμου, θα πρέπει να του δώσουμε είσοδο η οποία να είναι μοναδική για κάθε δείγμα του σετ δεδομένων ώστε να μπορεί ο αλγόριθμος να ομαδοποιήσει δεδομένα τα οποία ανήκουν στην ίδια κλάση, αλλά και να τα ξεχωρίσει απο αυτά των υπολοίπων. Η διαδικασία αυτή είναι γνωστή στον χώρο της υπολογιστικής όρασης και της επεξεργασίας εικόνας με τον όρο εξαγωγή χαρακτηριστικών. Όπως δηλώνει και το όνομά της, μέσω της διακασίας αυτής σε κάθε εικόνα εντοπίζονται συγκεκριμένα χαρακτηριστικά σημεία τα οποία την αντιπροσωπεύουν μοναδικά. Τα πιο συνηθισμένα τέτοιου είδους χαρακτηριστικά είναι τα \textlatin{SIFT}\textlatin{\cite{sift}} και τα \textlatin{HoG}\textlatin{\cite{hog}} τα οποία και εφαρμόστηκαν στην συγκεκριμένη περίπτωση λόγω καλύτερων αποτελεσμάτων σε αυτό το σετ δεδομένων. 
\par
Η βασική αρχή λειτουργίας της μεθόδου αυτής είναι ότι ένα συγκεκριμένο αντικείμενο μέσα σε μια εικόνα μπορεί να προσδιοριστεί απο την κατανομή της φωτεινότητας πάνω στα σημεία του αλλά και απο τον εντοπισμό της κλίσης της δηλαδή τον εντοπισμό των ακμών του αντικειμένου. Ο αλγόριθμος χωρίζει την εικόνα σε μικρές περιοχές απο \textlatin{pixel} και σε κάθε μια απο αυτές υπολογίζει την κλίση της φωτεινότητας δημιουργώντας ένα τελικό συνολικό ιστόγραμμα με τις τιμές αυτές. Το μέγεθος των περιοχών αυτών καθορίζεται απο το μέγεθος του πυρήνα ο οποίος αποτελεί παράμετρο του αλγορίθμου και καθορίζεται κατά την εκτέλεση του.

\par
Κατά το πρώτο πείραμα λοιπόν με αυτό το σετ δεδομένων μελετήθηκε τόσο η συμπεριφορά του αλγορίθμου \textlatin{LLE} αλλάζοντας τις παραμέτρους του όσο και η αποτελεσματικότητα των \textlatin{HoG}\textlatin{\cite{hog}} χαρακτηριστικών ανάλογα με το μέγεθος του πυρήνα. Πιο συγκεκριμένα για τις παραμέτρους του αλγορίθμου \textlatin{LLE} εξετάστηκαν οι τιμές $\mathbf{Κ=8,10,12}$ και $\mathbf{d=16,20,32,64,96,128,164,196,256}$ και για το μέγεθος του πυρήνα του αλγορίθμου \textlatin{HoG}\textlatin{\cite{hog}} οι τιμές $\mathbf{kernels=[2\times2],[4\times4],[8\times8]}$. Επίσης χρησιμοποιήθηκαν απο το σετ εκπαίδευσης \textbf{30.000} δείγματα, δηλαδή 3.000 εικόνες ψηφίων απο κάθε κλάση και το σύνολο των δεδομένων ταξινόμησης για την εξαγωγή του τελικού αποτελέσματος. Απο το πείραμα αυτό λοιπόν προκύπτουν τα παρακάτω αποτελέσματα

\selectlanguage{english}
\begin{table}[H]
\singlespacing
\centering
\label{tab:table11}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[2\times2]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
 & K=8 & K=10 & K=12 \\
\hline
d=16 & 20.81 & 20.52 & 20.56 \\
d=20 & 20.50 & 20.12 & 19.57 \\
d=32 & 19.69 & 19.10 & 19.06 \\
d=64 & 20.23 & 19.94 & 19.79 \\
d=96 & 20.71 & 20.11 & 19.78 \\
d=128 & 21.11 & 20.56 & 20.50 \\
d=164 & 21.29 & 20.83 & 20.69 \\
d=196 & 21.81 & 20.98 & 20.66 \\
d=256 & 22.25 & 21.60 & 21.05 \\
\hline
\end{tabular}
%\caption{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (HoG kernel [2x2])}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table12}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
 & K=8 & K=10 & K=12 \\
\hline
d=16 & 19.24 & 19.57 & 19.79 \\
d=20 & 18.22 & 18.16 & 18.35 \\
d=32 & 17.91 & 17.63 & \underline{17.42} \\
d=64 & 18.56 & 18.37 & 18.21 \\
d=96 & 18.90 & 18.72 & 18.73 \\
d=128 & 19.15 & 19.07 & 18.84 \\
d=164 & 19.68 & 19.33 & 19.22 \\
d=196 & 19.87 & 19.44 & 19.25 \\
d=256 & 20.30 & 19.79 & 19.58 \\
\hline
\end{tabular}
%\caption{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (HoG kernel [4x4])}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table13}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[8\times8]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
 & K=8 & K=10 & K=12 \\
\hline
d=16 & 23.36 & 24.38 & 25.10 \\
d=20 & 22.71 & 23.16 & 23.80 \\
d=32 & 22.52 & 22.62 & 22.50 \\
d=64 & 22.19 & 22.30 & 22.15 \\
d=96 & 22.39 & 22.19 & 22.15 \\
d=128 & 22.42 & 22.32 & 22.18 \\
d=164 & 22.47 & 22.33 & 22.35 \\
d=196 & 22.60 & 22.58 & 22.58 \\
d=256 & 23.01 & 22.61 & 22.47 \\
\hline
\end{tabular}
%\caption{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (HoG kernel [8x8])}
\end{table}

\selectlanguage{greek}
\par
Απο τα παραπάνω αποτελέσματα βλέπουμε ότι το μικρότερο σφάλμα δίνεται για τις παραμέτρους $\mathbf{K=12, d=32}$ του \textlatin{LLE} και $\mathbf{kernel=[4 \times 4]}$ των \textlatin{HoG} χαρακτηριστικών αντίστοιχα. Με αυτές τις βέλτιστες παραμέτρους λοιπόν περνάμε στην εκτέλεση του επόμενου πειράματος, στο οποίο θέλουμε να δούμε την αποτελεσματικότητα του αλγορίθμου \textlatin{LLE} ως προς την ταξινόμηση των δεδομένων αξιολόγησης σχετικά με το αποτέλεσμα της ταξινόμησης χωρίς μείωση των διαστάσεων. Επίσης με το πείραμα αυτό έχουμε σκοπό να συγκρίνουμε το βέλτιστο αποτέλεσμα μας μετά απο την μείωση των διαστάσεων με τα αποτελέσματα της συγκεκριμένης δημοσίευσης \href{http://ufldl.stanford.edu/housenumbers/nips2011\textunderscore housenumbers.pdf}{\textlatin{Reading Digits in Natural Images
with Unsupervised Feature Learning}} \cite{12} η οποία αποτελεί και την πρωτότυπη δημοσίευση του συγκεκριμένου σετ δεδομένων και είναι μια συνεργασία της \textlatin{Google} και του πανεπιστημίου του \textlatin{Stanford}. Τρέχοντας το πείραμα λοιπόν για τις βέλτιστες παραμέτρους που βρήκαμε παραπάνω, δηλαδή $\mathbf{K=12, d=32}$ και $\mathbf{kernel=[4 \times 4]}$, στο σύνολο των δεδομένων του \textlatin{SVHN} τα αποτελέσματα είναι εντυπωσιακά. Πιο συγκεκριμένα, εφαρμόζοντας την εξαγωγή των χαρακτηριστικών \textlatin{HoG} καταλήγουμε να έχουμε για κάθε εικόνα ένα διάνυσμα μεγέθους 1764 στοιχείων, απο τα οποία εφαρμόζοντας τον αλγόριθμο \textlatin{LLE} καταλήγουμε να κρατήσουμε 32 απο αυτά. Για το μέσο τετραγωνικό σφάλμα ταξινόμησης με την χρήση του αλγορίθμου \textlatin{k-NN}, με $k=8$ έχουμε για τις αρχικές διαστάσεις \textlatin{D} την τιμή \textbf{17.0\%} ενώ για τις τελικές διαστάσεις \textlatin{d} την τιμή \textbf{16.67\%}. Το γεγονός ότι μετά απο τεράστια μείωση των παραμέτρων έχουμε μια τόσο μικρή απόκλιση στο τελικό σφάλμα ταξινόμησης φανερώνει ότι η μείωση των διαστάσεων μπορεί να συμβάλει πολύ αποτελεσματικά στο κόστος των υπολογισμών κατά την διαδικασία της ταξινόμησης.

\selectlanguage{english}
\begin{table}[H]
\singlespacing
\centering
\label{tab:table14}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε ολόλκηρο το σύνόλο των δεδομένων εκπαίδευσης}
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|}
\hline
 & LLE dim-reduction & No dim-reduction \\
\hline
d=32 & & \\
K=12 & \underline{\textbf{16.67}} & 17.0 \\
k-NN - k=8 & & \\
\hline
\end{tabular}
%\caption{SVHN(full dataset) k-NN Classification Mean Average (\%) Error (HoG kernel [4x4])}
\end{table}

\selectlanguage{greek}
\par
Όπως έγινε φανερό και απο τα παραπάνω αποτελέσματα, ο αλγόριθμος \textlatin{LLE} είναι σε θέση να μειώσει δραματικά το κόστος υπολογισμού του βήματος της τελικής ταξινόμησης μέσω της μείωσης των διαστάσεων κατά έναν πολύ μεγάλο αριθμό. Παράλληλα όπως φάνηκε το σφάλμα μπορεί κυμαίνεται σε αμελητέα όρια γεγονός το οποίο σε πολλές πρακτικές εφαρμογές είναι επιθυμητό με στόχο να κερδίσουμε στον χρόνο υπολογισμού του τελικού αποτελέσματος αποφεύγοντας την ταξινόμηση στον χώρο των αρχικών διαστάσεων αλλά εφαρμόζοντάς την στον τελικό μειωμένο χώρο, για τον οποίο $d \ll D$. Επίσης, για την περίπτωση στην οποία θέλουμε αποτελέσματα ταξινόμησης για το σετ δεδομένων αξιολόγησης σε πραγματικό χρόνο εφαρμόζουμε την Μέθοδο-1, η οποία προυποθέτει ότι έχουμε τα δεδομένα μειωμένων διαστάσεων του σετ εκπαίδευσης, και έπειτα μπορούμε ταχύτατα στον χώρο με διαστάσεις \textlatin{d} να εφαρμόσουμε τον αλγόριθμο ταξινόμησης \textlatin{k-NN} έχοντας τα αποτελέσματα σε πραγματικό χρόνο. Το συγκεκριμένο πείραμα μάλιστα χρησιμοποιώντας 42.000 απο τα δεδομένα εκπαίδευσης και όλο το σετ αξιολόγησης (με τις βέλτιστες παραμέτρους που αναφέραμε παραπάνω και εφαρμόζοντας την Μέθοδο-1) έδωσε σαν αποτελέσματα τις τιμές 18.00\% για τις διαστάσεις \textlatin{D} και 18.34\% για τις διαστάσεις \textlatin{d}, σφάλμα ανεκτό αν αναλογιστεί κανείς την διαφορά στο κόστος υπολογισμού ενός προβλήματος με πολυπλοκότητα $ \mathcal{O}(N^2)$ με $N \simeq 70K$ (δεδομένα εκπαίδευσης) και $N \simeq 100K$ (δεδομένα εκπαίδευσης $+$ δεδομένα αξιολόγησης) αντίστοιχα.
\par
Ακόμα και αυτή η διαδικασία όμως της Μεθόδου-1, όπως αναφέραμε και σε προηγούμενη παραγραφο, σχεδόν σε όλες τις πρακτικές εφαρμογές λόγω της πολυπλοκότητας $ \mathcal{O}(N^2)$  του τελευταίου βήματος του αλγορίθμου είναι αδύνατο να εκτελεσθεί. Για τον λόγο αυτό, μελετήσαμε την συμπεριφορά της Μεθόδου-2 για διάφορες τιμές ως προς τον αριθμό των υποχώρων. Τα αποτελέσματα του πειράματος παρουσιάζουν πολύ μεγάλο ενδιαφέρον διότι μπορούμε να μειώσουμε δραματικά το κόστος υπολογισμού χωρίζοντας τον αρχικό χώρο σε μικρά υποσύνολα. Επίσης στο σημείο αυτό αξίζει να παρατηρηθεί η μεγάλη μείωση του σφάλματος στο τελικό αποτέλεσμα της ταξινόμησης μετά απο την διαδικασία της πλειοψηφικής επιλογής του τελικού αποτελέσματος. Συγκεκριμένα εκτελέσαμε το πείραμα αυτό χωρίζοντας το αρχικό σετ δεδομένων σε \textbf{3,5,10} και \textbf{20 υποχώρους}. Τα αποτελέσματα φαίνονται αναλυτικά στους παρακάτω πίνακες

\begin{table}[H]
\singlespacing
\centering
\label{tab:table15}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. Παράμετροι του αλγορίθμου \textlatin{LLE: K=12, d=32} και χωρισμός σε 3 υποσύνολα μέσω της Μεθόδου-2. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textlatin{Subspaces} & \textlatin{Subspace error} & \textlatin{Method-2} & \textlatin{D-dimensions}  \\
\hline
1 & 20.19 & & \\
2 & 19.05 & \underline{\textbf{18.30}} & 18.71 \\
3 & 19.97 & & \\
\hline
\end{tabular}
%\caption{\textlatin{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (LLE: K=12,d=32, HoG kernel [4x4]), Method-2 \textbf{3 Subspaces}}}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table16}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. Παράμετροι του αλγορίθμου \textlatin{LLE: K=12, d=32} και χωρισμός σε 5 υποσύνολα μέσω της Μεθόδου-2. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textlatin{Subspaces} & \textlatin{Subspace error} & \textlatin{Method-2} & \textlatin{D-dimensions}  \\
\hline
1 & 20.28 & & \\
3 & 21.11 & & \\
2 & 20.93 & \underline{\textbf{18.37}} & 18.61 \\
4 & 20.92 & & \\
5 & 21.11 & & \\
\hline
\end{tabular}
%\caption{\textlatin{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (LLE: K=12,d=32, HoG kernel [4x4]), Method-2 \textbf{5 Subspaces}}}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table17}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. Παράμετροι του αλγορίθμου \textlatin{LLE: K=12, d=32} και χωρισμός σε 10 υποσύνολα μέσω της Μεθόδου-2. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textlatin{Subspaces} & \textlatin{Subspace error} & \textlatin{Method-2} & \textlatin{D-dimensions}  \\
\hline
1 & 22.54 & & \\
2 & 22.51 & & \\
3 & 22.24 & & \\
4 & 22.33 & & \\
5 & 21.72 & \underline{\textbf{18.58}} & 18.52 \\
6 & 22.17 & & \\
7 & 22.71 & & \\
8 & 22.07 & & \\
9 & 22.21 & & \\
10 & 22.77 & & \\
\hline
\end{tabular}
%\caption{\textlatin{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (LLE: K=12,d=32, HoG kernel [4x4]), Method-2 \textbf{10 Subspaces}}}
\end{table}

\begin{table}[H]
\singlespacing
\centering
\label{tab:table18}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{SVHN} με τον αλγόριθμο κοντινότερων γειτόνων. Εξαγωγή \textlatin{HoG} χαρακτηριστικών με μέγεθος πυρήνα $[4\times4]$ σε 30.000 δείγματα του συνόλου εκπαίδευσης. Παράμετροι του αλγορίθμου \textlatin{LLE: K=12, d=32} και χωρισμός σε 20 υποσύνολα μέσω της Μεθόδου-2. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textlatin{Subspaces} & \textlatin{Subspace error} & \textlatin{Method-2} & \textlatin{D-dimensions}  \\
\hline
1 & 23.72 & & \\
2 & 23.40 & & \\
3 & 23.70 & & \\
4 & 24.93 & & \\
5 & 24.06 & & \\
6 & 23.26 & & \\
7 & 23.54 &  & \\
8 & 24.87 & & \\
9 & 23.01 &  & \\
10 & 23.39 & 19.13 & 18.52 \\
11 & 23.75 & & \\
12 & 24.52 &  & \\
13 & 24.77 &  & \\
14 & 24.89 & & \\
15 & 24.11 & & \\
16 & 23.86 & & \\
17 & 23.24 &  & \\
18 & 24.26 & & \\
19 & 23.62 &  & \\
20 & 25.07 &  & \\
\hline
\end{tabular}
%\caption{\textlatin{SVHN(30K train data) k-NN Classification Mean Average (\%) Error (LLE: K=12,d=32, HoG kernel [4x4]), Method-2 \textbf{20 Subspaces}}}
\end{table}

\subsection{Πειράματα - \textlatin{Arcene}}
\par
\href{http://archive.ics.uci.edu/ml/datasets/Arcene}{\textbf{\textlatin{Arcene:}}\textlatin{\cite{15}}} Τελευταίο σετ δεδομένων πάνω στο οποίο εφαρμόσαμε τον αλγόριθμο μη γραμμικής μείωσης διαστάσεων \textlatin{LLE} είναι το \textlatin{Arcene}\textlatin{\cite{15}}. Πρόκειται για δεδομένα προερχόμενα απο τον χώρο της Ιατρικής και συγκεκριμένα στόχος είναι να γίνει σωστή ταξινόμηση των ασθενών ανάλογα με το αν πρόκειται να εμφανίζουν κάποιας μορφής καρκίνο ή όχι. Τα δεδομένα αυτά έχουν προέλθει εφαρμόζοντας την τεχνική φασματομετρία μάζας σε ασθενείς οι οποίοι είτε είναι υγιείς είτε έχουν παρουσιάσει ήδη κάποιας μορφής καρκίνο, προχωρημένου ή και πρώιμου σταδίου. Οι μετρήσεις έγιναν απο τα κέντρα \textlatin{National Cancer Institute (NCI)} και \textlatin{ Eastern Virginia Medical School (EVMS)}, και πρόκειται για ένα σετ δεδομένων απο 900 ασθενείς με 10000 παραμέτρους για τον καθέναν. Όπως αναφέραμε και παραπάνω, ο στόχος του σετ αυτού είναι να γίνει ο σωστός διαχωρισμός των ασθενών ως προς την πρόβλεψη για το αν βρίσκονται στην ευπαθή ομάδα ή όχι. Για το συγκεκριμένο σετ δεδομένων, χρησιμοποιήσαμε τα δεδομένα εκπαίδευσης και τα δεδομένα αξιολόγησης (\textlatin{training and validation sets}) τα οποία περιέχουν 200 συνολικά δεδομένα. Απο αυτά χρησιμοποιήσαμε τα 150 ως δεδομένα εκπαίδευσης και τα 50 υπόλοιπα ως σετ αξιολόγησης. Τα αποτελέσματα για όλους τους συνδυασμούς των παραμέτρων φαίνονται στον παρακάτω πίνακα

\begin{table}[H]
\singlespacing
\centering
\label{tab:table19}
\selectlanguage{greek}
\caption{Μέσο (\%) σφάλμα ταξινόμησης του σετ δεδομένων \textlatin{Arcene} με τον αλγόριθμο κοντινότερων γειτόνων. Σφάλμα ταξινόμησης στον χώρο των αρχικών διαστάσεων \textlatin{(D=10.000)} ίσο με 24\%. }
\selectlanguage{english}
\vspace*{5mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 & K=10 & K=12 & K=16 & K=20 & K=24 & K=32 & K=64 \\
\hline
\textlatin{d}=10 & \underline{\textbf{10}} & \underline{\textbf{12}} & \underline{\textbf{18}} & \underline{\textbf{18}} & \underline{\textbf{20}} & \underline{\textbf{18}} & \underline{\textbf{18}} \\
\textlatin{d}=16 & \underline{\textbf{14}} & \underline{\textbf{22}} & \underline{\textbf{16}} & \underline{\textbf{22}} & \underline{\textbf{18}} & \underline{\textbf{18}} & \underline{\textbf{18}} \\
\textlatin{d=20} & \underline{\textbf{16}} & \underline{\textbf{18}} & \underline{\textbf{16}} & \underline{\textbf{16}} & \underline{\textbf{22}} & \underline{\textbf{16}} & \underline{\textbf{16}} \\
\textlatin{d}=24 & \underline{\textbf{14}} & \underline{\textbf{14}} & \underline{\textbf{18}} & \underline{\textbf{20}} & \underline{\textbf{18}} & \underline{\textbf{18}} & \underline{\textbf{16}} \\
\textlatin{d}=32 & \underline{\textbf{14}} & \underline{\textbf{20}} & 24 & \underline{\textbf{18}} & \underline{\textbf{20}} & \underline{\textbf{18}} & \underline{\textbf{18}} \\
\textlatin{d}=40 & \underline{\textbf{16}} & \underline{\textbf{14}} & \underline{\textbf{14}} & \underline{\textbf{16}} & \underline{\textbf{16}} & \underline{\textbf{22}} & \underline{\textbf{18}} \\
\textlatin{d}=52 & \underline{\textbf{10}} & \underline{\textbf{16}} & \underline{\textbf{14}} & \underline{\textbf{14}} & \underline{\textbf{16}} & \underline{\textbf{16}} & \underline{\textbf{14}} \\
\textlatin{d}=64 & \underline{\textbf{14}} & \underline{\textbf{16}} & \underline{\textbf{22}} & \underline{\textbf{20}} & \underline{\textbf{22}} & \underline{\textbf{18}} & \underline{\textbf{14}} \\
\textlatin{d}=96 & \underline{\textbf{22}} & \underline{\textbf{22}} & \underline{\textbf{22}} & \underline{\textbf{18}} & \underline{\textbf{12}} & \underline{\textbf{16}} & \underline{\textbf{22}} \\
\textlatin{d}=128 & 24 & \underline{\textbf{10}} & \underline{\textbf{26}} & \underline{\textbf{14}} & 28 & 28 & 26 \\
\hline
\end{tabular}
%\caption{\textlatin{Arcene Dataset LLE Dimensionality Reduction and kNN Classification (k=4). \textbf{No dimensionality reduction (D=10.000) Classification error: 24\%}}}
\end{table}

\par
Όπως είναι φανερό απο τον παραπάνω πίνακα, στις περισσότερες απο τις περιπτώσεις μετά απο την μείωση των διαστάσεων εφαρμόζοντας τον αλγόριθμο \textlatin{LLE} έχουμε πολύ μεγάλη αύξηση του αποτελέσματος ορθής ταξινόμησης. Το βέλτιστο αποτέλεσμα δίνεται για τις παραμέτρους ($\mathbf{K=10, d=10}$), ($\mathbf{K=10,d=52}$) και ($\mathbf{K=12,d=128}$) και βλέπουμε ότι έχουμε μείωση του σφάλματος κατά \textbf{10\%} σε σχέση με αυτό του αρχικού χώρου των 10.000 διαστάσεων. Το γεγονός αυτό, σε συνδυασμό με την δραματική μείωση στο κόστος των υπολογισμών κατά την διαδικασία της ταξινόμησης, απο 10.000 παραμέτρους έχουμε κρατήσει μόνο \textbf{10, 52} και \textbf{128} για τις παραπάνω περιπτώσεις αντίστοιχα, έρχεται να αποδείξει για άλλη μια φορά ότι η διαδικασία της μείωσης των διαστάσεων μπορεί να έχει πολλαπλά οφέλη τόσο στην βελτίωση του αποτελέσματος όσο και στον χρόνο που απαιτείται για τον υπολογισμό του. Τέλος αποτελεί πολύ μεγάλο ενδιαφέρον στο συγκεκριμένο πείραμα το αποτέλεσμα του σφάλματος \textbf{10\%} για τελικό αριθμό διαστάσεων \textbf{\textlatin{d}=10} απο τις αρχικές \textbf{\textlatin{D}=10.000}.