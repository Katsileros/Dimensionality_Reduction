

%\addcontentsline{toc}{chapter}{Πειράματα}

\chapter{Πειράματα}
\par
Στην εργασία αυτή πραγματοποιήθηκε μια σειρά πειραμάτων τα οποία έχουν ως στόχο τόσο την διερεύνηση του αλγορίθμου ως προς την απόδοσή του σε εφαρμογές αναγνώρισης προτύπων όσο και στο πώς επιδρούν στο αποτέλεσμα του αλγορίθμου οι δύο παράμετροι που δέχεται σαν είσοδο. Οι παράμετροι αυτοί είναι ο αριθμός $k$ των κοντινών γειτόνων σύμφωνα με τους οποίους κατασκευάζεται ο πίνακας γειτνίασης του πρώτου βήματος του αλγορίθμου και η δεύτερη παράμετρος είναι ο αριθμός $d$ των τελικών διαστάσεων και οι οποίες καθορίζουν ουσιαστικά το αποτέλεσμα του τελευταίου βήματος. Ο σχεδιασμός των πειραμάτων έγινε με τρόπο ώστε να γίνει φανερό το πως επηρεάζουν την συμπεριφορά του αλγορίθμου οι παράμετροι αυτοί αλλά επίσης δόθηκε έμφαση στο να βρεθεί ο βέλτιστος συνδυασμός των δύο ανάλογα με το σετ δεδομένων κάθε πειράματος. 
\par
Eφαρμόζονται επίσης δύο τεχνικές με τις οποίες μπορεί κάποιος να αποφύγει το τεράστιο υπολογιστικό κόστος που απαιτείται. Συγκεκριμένα το τελευταίο βήμα του αλγορίθμου το οποίο είναι και το πιο απαιτητικό έχει πολυπλοκότητα $\mathcal{O}n^{3}$ στην γενική περίπτωση ενώ στην συγκεκριμένη περίπτωση λόγω του αραιού μητρώου $M$ είναι της τάξης $\mathcal{O}n^{2}$. Αντιλαμβανόμαστε λοιπόν ότι ακόμα και για ένα σχετικά μικρό σετ δεδομένων, για τα δεδομένα του κλάδου της μηχανικής μάθησης, το πρόβλημα που έχουμε να αντιμετωπίσουμε έχει απαγορευτικές διαστάσεις. 
\par
Ένα άλλο πρόβλημα που συναντάει κανείς κατά την εφαρμογή του αλγορίθμου σε κάποιο σετ δεδομένων είναι ο εξής περιορισμός. Ας υποθέσουμε ότι έχουμε ένα σύνολο δεδομένων μεγέθους $N$, απο τα οποία για κάποιον αριθμό $N1$ απο αυτά γνωρίζουμε την ετικέτα τους. Με τον όρο ετικέτα εννοούμε την τελική κλάση στην οποία ανήκει το κάθε δείγμα. Για τα υπόλοιπα δείγματα, έστω μεγέθους $N2$ δεν γνωρίζουμε την ετικέτα τους και είναι αυτά τα δείγματα για τα οποία θέλουμε να εξάγουμε το συμπέρασμα. Το συμπέρασμα αυτό είναι φυσικά η τελική απόφαση ως προς σε ποιά κλάση θα πρέπει να ταξινομηθεί το καθένα απο αυτά. Προφανώς η παραπάνω απόφαση προκύπτει λαμβάνοντας υπόψιν την πληροφορία την οποία μας δίνει το σύνολο των δεδομένων $N1$ τα οποία στον χώροτ της μηχανικής μάθησης αναφέρονται ως το σύνολο των δεδομένων εκπάιδευσης \textlatin{(train data)}. Τα υπόλοιπα δείγματα $N2$ αναφέρονται ως το σύνολο των δεδομένων αξιολόγησης \textlatin{(test data)}.
\par
Στο συγκεκριμένο λοιπόν έστω οτι τα δείγματα του αρχικού χώρου έχουν αρχική διάσταση μεγέθους $D$ και μέσω του αλγόριθμου μείωσης των διαστάσεων θέλουμε να βρεθούμε σε έναν νέο χώρο διάστασης $d$, προφανώς με $d < D$. Στην περίπτωση αυτή λοιπόν ο πιο απλός συλλογισμός που θα μπορούσε να κάνει κάποιος είναι να τρέξει τον αλγόριθμο \textlatin{LLE} πάνω στο σετ δεδομένων εκπαίδευσης ώστε να έχει ένα σύνολο δεδομένων μεγέθους $Ν1$, διάστασης $d$. Με τον ίδιο ακριβώς τρόπο θα μπορούσε να έχει και το δεύτερο σετ δεδομένων, τα δεδομένα αξιολόγησης, μεγέθους $N2$ και αυτά διάστασης $d$. Έπειτα για την ταξινόμηση των αποτελεσμάτων θα μπορούσε να εφαρμοστεί ο αλγόριθμος ταξινόμησης κοντινότερων γειτόνων ανάμεσα στο σετ αξιολόγησης με το σετ εκπαίδευσης. Έπειτα ανάλογα με την κλάση στην οποία ανήκει ο κοντινότερος γείτονας απο το σετ εκπαίδευσης για κάθε ένα στοιχείο των δεδομένων αξιολόγησης θα καταλήγαμε στην τελική απόφαση για την κλάση στην οποία ανήκει κάθε ένα απο τα δεδομένα του σετ $N2$. Προφανώς ο αλγόριθμος κοντινότερων γειτόνων θα εφασρμοστεί στον χώρο μειωμένης διάστασης μεγέθους $d$ χρησιμοποιώντας για παράδειγμα την μετρική της Ευκλείδιας απόστασης μεταξύ των σημείων.
\par
Αν λοιπόν εφαρμόσουμε την παραπάνω διαδικασία για κάποιο σετ δεδομένων, θα παρατηρήσουμε ότι το τελικό αποτέλεσμα της ταξινόμησής μας έχει πολύ μικρή επιτυχία. Αυτό συμβαίνει διότι, οι δύο υποχώροι οι οποίοι προέκυψαν απο το τελικό βήμα του αλγορίθμου \textlatin{LLE}, κατά το οποίο υπολογίστηκε ο νέος χώρος μειωμένης διάστασης για κάθε ένα απο τα δύο σύνολα δεδομένων, έχουν διαφορετική διανυσματική βάση και δεν μπορούν σε καμιά περίπτωση να συσχετιστούν μεταξύ τους ώστε να μπορέσουμε απο τα δεδομένα του ενός να καταλήξουμε σε κάποιο ορθό συμπέρασμα για τα δεδομένα του άλλου. Ο παραπάνω λοιπόν περιορισμός μας αναγκάζει να εφαρμόσουμε τον αλγόριθμο μείωσης των διαστάσεων στο σύνολο των δεδομένων, δηλαδή δίνοντας σαν είσοδο στον αλγόριθμο το σύνολο των δεδομένων μεγέθους $N = N1+N2$. Με τον τρόπο αυτό θα καταλήγαμε σε ένα νέο σετ δεδομένων μεγέθους $N$ αλλά διάστασης $d < D$. Τέλος σε αυτό το σετ δεδομένων τώρα μπορούμε κάλλιστα να εφαρμόσουμε τον αλγόριθμο εύρεσης κοντινότερων γειτόνων για κάθε ένα απο τα δεδομένα αξιολόγησης ως πρός τα δεδομένα εκπαίδευσης, φυσικά στον χώρο $d$ διστάσεων, και έτσι να καταλήξουμε στην ορθή ταξινόμηση των δειγμάτων $N2$ ως προς την κλάση στην οποία ανήκουν. 

\section{Μέθοδοι αντιμετώπισης της πολυπλοκότητας του προβλήματος}
\par
Όπως αντιλαμβανόμαστε απο την παραπάνω ανάλυση, η διαδικασία αυτή δεν είναι καθόλου πρακτική και μάλιστα δεν δίνει την δυνατότητα για λήψη αποφάσεων και ταξινόμησης δειγμάτων σε πραγματικό χρόνο. Αυτό διότι, για κάθε δείγμα αξιολόγησης που μας έρχεται ως είσοδος κάποια συγκεκριμένη χρονική στιγμή, και για το οποίο θέλουμε να καταλήξουμε σε κάποιο συμπέρασμα ως προς την κλάση στην οποία ανήκει, θα πρέπει να το ενσωματώνουμε στο σετ των δεδομένων εκπαίδευσης και στην συνέχεια να εκτελούμε τον αλγόριθμο \textlatin{LLE}. Αντιλαμβανόμαστε λοιπόν ότι η συγκεκριμένη διαδικασία δεν προσφέρεται σε καμιά περίπτωση για πρακτικές εφαρμογές κατα τις οποίες μάλιστα ο στόχος μας είναι να γίνει μείωση των διαστάσεων ώστε να μπορούμε να λαμβάνουμε ταχύτερα και ακριβέστερα αποτελέσματα. Το γεγονός αυτό μάλιστα αντιτίθεται στην συνολική φιλοσοφία της μείωσης των διαστάσεων κατά την οποία η μείωση των διαστάσεων μπορεί να επιταχύνει σε πολύ μεγάλο βαθμό τους απαραίτητους υπολογισμούς.

\subsection{Μέθοδος-1: Προβολή στον χώρο των δεδομένων εκπαίδευσης}
\par
Για τους λόγους λοιπόν οι οποίοι και αναφέρθηκαν παραπάνω, προκύπτει η ανάγκη να βρεθεί κάποιος τρόπος με τον οποίο να μπορούμε οποιαδήποτε στιγμή θέλουμε να ταξινομήσουμε κάποιο δεδομένο, χρησιμοποιώντας βέβαια την πληροφορία που μπορούν να μας δώσουν τα δεδομένα του σετ εκπαίδευσης. Την λύση στο πρόβλημα αυτό λοιπόν έρχεται να δώσει η λογική με την οποία λειτουργεί ο αλγόριθμος \textlatin{LLE}. Πιο συγκεκριμένα όπως αναφέραμε παραπάνω, η μέθοδος αυτή έχει ως στόχο να διατηρήσει τα γεωμετρικά τοπικά χαρακτηριστικά για κάθε ένα απο τα δείγματα του συνόλου εκπαίδευσης τόσο στον χώρο των αρχικών διαστάσεων όσο και στον τελικό χώρο μειωμένης διάστασης. Επίσης, λόγω του ότι η ενσωμάτωση των δεδομένων στον χώρο μειωμένης διάστασης γίνεται με τη χρήση του πίνακα βαρών $W$, ο οποίος προσδιορίζει για κάθε δείγμα του αρχικού χώρου την ανακατασκευή του μέσω των κοντινών του γειτόνων προκύπτει και η ιδέα της μεθόδου αυτής. Ο αντίστοιχος ψευδοκώδικας είναι ο παρακάτω
\selectlanguage{english}
\begin{algorithm}
   \caption{Projection Method}
    \begin{algorithmic}[1]   	
    	\State Let $Xtrain$ be $[D \times N1]$ Train\textunderscore dataset matrix and $Xtest$ be $[D \times N2]$ Test\textunderscore dataset matrix \Comment{N1,N2 declare the number of data and D the number of dimensions}
		\State 
    	
    	\State Let matrix $Y$ be $[d \times N1]$ Train data, after dimensionality reduction \Comment{ d < D}
    	\State
    	
    	\State Let matrix nn\textunderscore graph with size $[N1 \times N2]$ and all elements equal to zero
    	\State 
        
        \For{$i = 1$ to ${N_2}$}
            \State Find K-Nearest Neighbors from $Xtrain$
        \EndFor
        \State
		
		\State Keep the results to matrix IDX with size $[N2 \times K]$ \Comment{ K is the number of nearest neighbors}
		\For{$i = 1$ to ${N_2}$}
            \State Set IDX(i,1:K) cells of nn\textunderscore graph matrix equal to ones
            \State Make the matrix multiplication $Y \times$ nn\textunderscore graph$(1:N1,i)$ and store the result to $testY(1:d,i)$ \Comment{testY(:,i) is the result of dimensionality reduced $Xtest_i$}
        \EndFor
        \State 
        
        \State Final matrix testY has size $[d \times N2]$ and represents the projection of Xtest D-dimensional data into the d-dimensional emndedding subspace.  
        \State
        \State Now execute K-NN Classification between testY and Y datasets, to the d-dimensional space
	\end{algorithmic}
\end{algorithm}
\selectlanguage{greek}
\par
Απο την παραπάνω ανάλυση της μεθόδου γίνεται φανερό ότι μπορούμε να χρησιμοποιήσουμε την πληροφορία των δεδομένων Υ, τα οποία είναι τα δεδομένα εκπαίδευσης \textlatin{Xtrain} στον χώρο μειωμένης διάστασης, ώστε να ταξινομήσουμε οποιοδήποτε δείγμα απο το σετ δεδομένων αξιολόγησης. Το γεγονός αυτό, σε συνδυασμό με την ελάχιστη αύξηση του σφάλματος ταξινόμησης όπως θα γίνει φανερό στα αποτελέσματα των πειραμάτων κάνει την μέθοδο αυτή πολύ ελκυστική για πρακτικές εφαρμογές οι οποίες απαιτούν αποτελέσματα σε πραγματικό χρόνο και μάλιστα σε πολύ μεγάλες ταχύτητες γεγονός το οποίο εξασφαλίζεται απο τους υπολογισμούς στον χώρο των μειωμένωνδιαστάσεων $d$. Όπως μπορούμε να παρατηρήσουμε στο πρώτο βήμα της μεθόδου εφαρμόζουμε τον αλγόριθμο κοντινότερων γειτόνων στον χώρο των αρχικών διαστάσεων μεγέθους $D$, για το σετ δεδομένων αξιολόγησης ως προς το σετ δεδομένων εκπαίδευσης. Επομένων θα μπορούσαμε να ισχυριστούμε ότι το υπόλοιπο της διαδικασίας είναι περιττό εκτώς και αν το τελικό αποτέλεσμα ταξινόμησης είναι καλύτερο απο αυτό στον αρχικό χώρο. Παρ'όλα αυτά αν αναλογιστούμε ένα πολύ μεγάλο σετ δεδομένων με έναν αρκετά μεγάλο αριθμό διαστάσεων, όπως πρόκειται άλλωστε για τα περισσότερα πραγματικά σετ στον χώρο της αναγνώρισης προτύπων, τότε το γεγονός οτι απο το σημείο αυτό και μετά μπορούμε την πληροφορία των $D$ διαστάσεων να την πάρουμε απο τις πολύ λιγότερες όπως θα δούμε $d$ τελικές διαστάσεις αποτελεί τεράστιο κέρδος τόσο σε επόμενους υπολογισμούς όσο και στους όρους που απαιτούνται για την διαχείριση των δεδομένων.  
\par
Στην παραπάνω διαδικασία θεωρήσαμε ότι έχουμε ήδη εφαρμόσει τον αλγόριθμο \textlatin{LLE} στο σετ δεδομένων εκπαίδευσης και έτσι έχουμε το αποτέλεσμα, δηλαδή τα δεδομένα στον χώρο μειωμένης διάστασης στον πίνακα \textlatin{Y}. Το βήμα αυτό, παρ'ότι είναι σαφώς οικονομικότερο απο την περίπτωση στην οποία θα εφαρμόζαμε στον αλγόριθμο στο σύνολο των δεδομένων εκπαίδευσης αλλά και των δεδομένων αξιολόγησης, στις περισσότερες εφαρμογές απαιτεί πολύ μεγάλο μέγεθος μνήμης γεγονός που καθιστά τις περισσότερες φορές αδύνατη την εκτέλεση του αλγορίθμου. Για να διευκρινιστεί η διαδικασία εφαρμογής της μεθόδου αλλά και οι διαστάσεις των πινάκων κάθε βήματος δίνεται το παρακάτω διάγραμμα  
\newpage
\begin{figure}[t!]
\centering
\includegraphics[scale=0.8]{figs/5.png}
\newline
\caption{ \textlatin{Method-1: Project test data, using adjacency matrix}.} 
\end{figure}
\newpage

\subsection{Μέθοδος-2: Δημιουργία υποχώρων και πλειοψηφική απόφαση ταξινόμησης }
\par
Για την αντιμετώπιση λοιπόν του παραπάνω προβλήματος μπορούμε να εφαρμόσουμε την Μέθοδο-2. Η βασική ιδέα της μεθόδου αυτής, όπως θα δούμε αναλυτικά και στον ψευδοκώδικα παρακάτω, είναι να διασπάσει το αρχικό σετ δεδομένων σε υποσύνολα απο τα οποία στην συνέχεια συνδυάζει την πληροφορία που δίνει το καθένα και εξάγει το τελικό αποτέλεσμα ταξινόμησης. Σημαντικό σημείο στην διαδικασία αυτή είναι η κατασκευή των υποσυνόλων να γίνει με τρόπο τέτοιο ώστε το καθένα απο αυτά να περιέχει την ίδια ποσότητα πληροφορίας, με την έννοια ότι θα πρέπει ο διαμοιρασμός των δειγμάτων κάθε κλάσης να γίνει ομοιόμορφα σε όλα τα υποσύνολα. Με τον τρόπο αυτό στην πραγματικότητα επιλύονται πολλά μικρά υποπροβλήματα όμοια με το αρχικό. Υποπροβλήματα δηλαδή τα οποία περιέχουν την ίδια πληροφορία με το αρχικό σετ δεδομένων εκπαίδευσης αλλά σε μικρότερη ποσότητα. Αν προσέξουμε ώστε το κάθε υποσύνολο να περιέχει αρκετά δείγματα ώστε να μπορέσει να διατηρηθεί το λείο της πολλαπλότητας το οποίο είναι απαίτηση του αλγορίθμου \textlatin{LLE} τότε το αποτέλεσμα της λύσης κάποιου υποχώρου θα είναι πολύ κοντά σε αυτό του αρχικού προβλήματος. Συνδυάζοντας την πληροφορία των υποχώρων στν συνέχεια, και καταλήγοντας στο αποτέλεσμα της ταξινόμησης ανάλογα με την πλειοψηφία των αποτελεσμάτων όλων των υποχώρων το κέρδος είναι διπλό. Μειώνεται καταρχήν δραματικό το κόστος υπολογισμού του αλγορίθμου \textlatin{LLE} λόγω της μείωσης κατά μεγάλο βαθμό των δειγμάτων στα οποία εφαρμόζεται. Επίσης με την διαδικασία του ψηφίσματος και της πλειοψηφικής τελικής επιλογής βελτιώνεται κατά πολύ το αποτέλεσμα της ταξινόμησης σε σχέση με αυτό του κάθε υποχώρου ξεχωριστά. 
\par
Για να γίνει κατανοητός ο τρόπος κατασκευής των υποσυνόλων αλλά και της συνολικής διαδικασίας της μεθόδου δίνεται ένα γράφημα το οποίο αναπαριστά τον διαμοιρασμό των δειγμάτων και έπειτα ο συνολικός ψευδοκώδικας τηε μεθόδου. Στο παρακάτω γράφημα έστω ότι το αρχικό μου σύνολο δεδομένων είναι το γνωστό σύνολο δεδομένων \href{http://yann.lecun.com/exdb/mnist/}{\textlatin{MNIST}} με μέγεθος έστω \textlatin{N} και το οποίο περιέχει δεδομένα τα οποία ανήκουν σε δέκα κλάσεις (ψηφία απο το 0 εως το 9). Επίσης κάθε εικόνα έχει γίνει μετατροπή σε ένα διάνυσμα-στήλη μεγέθους $[Width \times Height]$, έστω $D$. Έστω επίσης για το συγκεκριμένο παράδειγμα ότι επιλέγουμε να το χωρίσουμε σε 3 υποσύνολα. Η διαδικασία διαμοιρασμού των δειγμάτων φαίνεται γραφικά παρακάτω
\newpage
\begin{figure}[t!]
\centering
\includegraphics[scale=0.8]{figs/6.png}
\newline
\caption{ \textlatin{Split main dataset into sub-datasets}.} 
\end{figure}
\newpage
\par
Όπως φαίνεται αναλυτικά στο παραπάνω σχήμα το σύνολο των αρχικών κλάσεων ομαδοποιείται και στην συνέχεια μοιράζονται ανάλογα τα δείγματα κάθε κλάσης, ομοιόμορφα, σε όσα υποσύνολα έχουμε επιλέξει. Απο το σημείο αυτό λοιπόν μπορούμε πλέον να εφαρμόσουμε τον αλγόριθμο μέιωσης διστάσεων, \textlatin{LLE}, σε κάθε ένα απο τα τελικά υποσύνολα δεδομένων καταλήγοντας σε τρείς νέους χώρους μειωμένης προφανώς διάστασης. Να διευκρινιστεί στο σημείο αυτό ότι η διαδικασία προβολής των δεδομένων αξιολόγησης μπορεί να γίνει είτε ενσωματώνοντάς τα σε κάθε ένα απο τα τρία τελικά σετ δεδομένων πρίν την εφαρμογή του αλγορίθμου ή να εφαρμοστεί η Μέθοδος-1. Σύμφωνα με την Μέθοδο-1, όπως εξηγήσαμε και παραπάνω, θα γίνει μείωση των διαστάσεων για κάθε υποσύνολο και στην συνέχεια για κάθε ένα χωριστά θα γίνει η προβολή των δεδομένων αξιολόγησης στον χώρο μειωμένης διάστασης του καθενός. 
\par
Αφού εφαρμοστεί μια απο τις παραπάνω μεθόδους, ανεξαρτήτως ποια, μπορούμε πλέον για κάθε ένα σύνολο δεδομένων (τελικά σετ εκπαίδευσης ένα εως τρία και σετ αξιολόγησης στον χώρο μειωμένης διάστασης $d$) να εφαρμόσουμε τον αλγόριθμο κοντινότερων γειτόνων \textlatin{(k-NN)} και να κάνουμε την ταξινόμηση κάθε δείγματος του σετ αξιολόγησης, στην κλάση εκτίμησης για κάθε έναν απο τους τελικούς υποχώρους. Τέλος, πλειοψηφικά αποφασίζουμε σε ποιά κατηγορία ανήκει το κάθε δείγμα, λαμβάνοντας υπόψιν την ψήφο ως προς την κλάση ταξινόμησης του δείγματος απο τους τρείς υποχώρους. Με τον τρόπο αυτό, όπως θα φανεί και στα πειράματα παρακάτω, βελτιώνεται σε πολύ μεγάλο βαθμό το τελικό αποτέλεσμα της ταξινόμησης σε σχέση με αυτό των τριών υποχώρων. Η διαδικασία αυτή, παρουσιάζεται και γραφικά στο παρακάτω σχήμα
\newpage
\begin{figure}[t!]
\centering
\includegraphics[scale=0.8]{figs/7.png}
\newline
\caption{ \textlatin{Sub datasets Dimensionality reduction and subspace classification voting}.} 
\end{figure}
\newpage

\section{Εφαρμογή του αλγορίθμου \textlatin{LLE} και των μεθόδων σε πραγματικά σετ δεδομένων}
\par
Στην εργασία αυτή δόθηκε έμφαση στην μελέτη του αλγορίθμου σε συγκεκριμένα σετ δεδομένα αλλά παράλληλα διερευνήθηκε σε μεγάλο βαθμό το πως επηρεάζουν την συμπεριφορά του οι παράμετροι που δέχεται ως είσοδο απο τον χρήστη. Αυτές είναι ο αριθμός των κοντινότερων γειτόνων \textlatin{(K)} για το πρώτο βήμα του αλγορίθμου και ο αριθμός των τελικών διστάσεων \textlatin{(d)} για το τελικό βήμα του. Επίσης, γίνεται σύγκριση στην απόδοση του αλγορίθμου ταξινόμησης κοντινότερων γειτόνων \textlatin{(k-NN)} για τον χώρο αρχικών διαστάσεων \textlatin{D} και αυτού των τελικών \textlatin{d}. 

\subsection{Πειράματα και Αποτελέσματα}
\par
\href{http://yann.lecun.com/exdb/mnist/}{\textbf{\textlatin{MNIST: }}}Το πρώτο σετ δεδομένων το οποίο χρησιμοποιήθηκε είναι το πολύ γνωστό και ευρέως χρησιμοποιούμενο σετ δεδομένων στον χώρο της αναγνώρισης προτύπων, \href{http://yann.lecun.com/exdb/mnist/}{\textlatin{MNIST}}. Το σετ αυτό αποτελείται απο 70.000 εικόνες, διάστασης $[28 \times 28]$ \textlatin{pixel}, οι οποίες περιέχουν χειρόγραφα ψηφία. Οι 60.000 απο αυτές ανήκουν στο σετ εκπαίδευσης και οι 10.000 στο σετ αξιολόγησης. Για την είσοδο των δεδομένων στον αλγόριθμο, εφαρμόστηκε η λεξικογραφική διάταξη σε κάθε μια απο τις εικόνες, καταλήγοντας σε ένα διάνυσμα διάστασης $D = 784$.
\subsubsection{Πειράματα - \textlatin{MNIST}}
\par
Στο πρώτο πείραμα με αυτό το σετ δεδομένων (ο κώδικας βρίσκεται στον φάκελο \textlatin{LLE\textunderscore MNIST}) διερευνήθηκε αρχικά η συμπεριφορά του αλγορίθμου ως προς την απόδοση του αποτελέσματος ταξινόμησης μετά την μείωση των διαστάσεων. Οι παράμετροι οι οποίες αξιολογήθηκαν είναι ο αριθμός Κ των κοντινότερων γειτόνων, ο αριθμός των τελικών διαστάσεων \textlatin{d} αλλά και ο αριθμός των υποσυνόλων της μεθόδου-2. Συγκεκριμένα για τον αριθμό κοντινότερων γειτόνων του πρώτου βήματος του αλγορίθμου δόθηκαν οι τιμές $K = 6,7,8,9,10,12,16,20,24,32,64$, για τον αριθμό των τελικών διαστάσεων του τελευταίου βήματος οι τιμές $d = 10,16,20,24,32,40,52,64,96,128,256$ και για τον αριθμό των δειγμάτων των υποσυνόλων οι τιμές $batch\textunderscore size = 10.000,20.000,60.000$ .Δηλαδή χωρίσαμε το σετ δεδομένων εκπαίδευσης των 60.000 εικόνων σε 6,3,1 υποσύνολα αντίστοιχα. Για το πρώτο αυτό πείραμα δεν χρησιμοποιήσαμε την μέθοδο-1, δηλαδή σε κάθε υποσύνολο κάθε φορά ενσωματώναμε τα δεδομένα αξιολόγησης και στην συνέχεια εφαρμόζαμε τον αλγόριθμο για την μείωση των διαστάσεων στο σύνολο των δεδομένων εκπαίδευσης του υποχώρου συν τα δεδομένα αξιολόγησης. Για την εξαγωγή του τελικού αποτελέσματος ταξινόμησης εφαρμόστηκε ο αλγόριθμος \textlatin{k-NN} με $k=2$. Το μικρότερο ποσοστό σφάλματος ταξινόμησης δόθηκε για τις παραμέτρους $K=12, d=128, batch\textunderscore size=60.000$, με τιμή 3.06\% έναντι του 3.5\% το οποίο είναι το σφάλμα ταξινόμησης στον χώρο των αρχικών διαστάσεων \textlatin{D}. Πολύ μεγάλο ενδιαφέρον παρουσιάζει το γεγονός ότι το σφάλμα για τις παραμέτρους $K=8, d=10, batch\textunderscore size=60.000$ ισούτε με 3.31\% το οποίο είναι και αυτό μικρότερο απο την ταξινόμηση χωρίς την μείωση διαστάσεων. Αξίζει να δοθεί έμφαση στο γεγονός ότι έχουμε καλύτερο ποσοστό ταξινόμησης έπειτα απο δραματική μείωση των διαστάσεων, και συγκεκριμένα απο τις 784 αρχικές καταλήγουμε σε 10 τελικές. 
\par
Τρέχοντας την συνάρτηση οπτικοποίησης των αποτελεσμάτων (\textlatin{Visualization.m}) μπορεί να παρατηρήσει κανείς ότι για την περίπτωση όπου έχουμε 64 ή περισσότερες τελικές διαστάσεις το τελικό ποσοστό σφάλατος είναι μικρότερο ή οριακά ίσο και για την περίπτωση στην οποία έχουμε $batch_size=20.000$. Το γεγονός αυτό δίνει ένα πολύ μεγάλο πλεονέκτημα διότι χωρίζοντας το σετ δεδομένων εκπαίδευσης των 60.000 δειγμάτων, σε 3 υποσύνολα έχουμε τεράστια μείωσης στο κόστος των υπολογισμών αλλά και στην διαθέσιμη μνήμη η οποία απαιτείται για την εκτέλεση του αλγορίθμου. Ακόμα πιο ενδιαφέρον είναι το αποτέλεσμα του πειράματος με παραμέτρους $K=10, d=128, batch\textunderscore size=10.000$ για το οποίο έχουμε σφάλμα ταξινόμησης 3.31\%, αποτέλεσμα μικρότερο απο αυτό των αρχικών διαστάσεων αλλά και πολύ οικονομικότερο απο άποψη χρόνων και υπολογισμών διότι έχουμε χωρίζει το σετ εκπαίδευσης σε 6 μικρούς σχετικά υποχώρους οι οποίοι μειώνουν την πολυπλοκότητα επίλυσης του προβλήματος κατά πολλές τάξεις μεγέθους. Στα παρακάτω γραφήματα, τα οποία δείχνουν τα αποτελέσματα για συγκεκριμένους συνδυασμούς των παραμέτρων, φανερώνουν σημαντικά στοιχεία για την αποτελεσματικότητα της μείωσης των διαστάσεων στην συγκεκριμένη εφαρμογή ταξινόμησης του σετ δεδομένων \textlatin{MNIST}.
\newpage
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/8.png}
\newline
\caption{ \textlatin{Classification Results with final dimensions d=10 (Method-2)}.} 
\end{figure}

\newpage
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/9.png}
\newline
\caption{ \textlatin{Classification Results with final dimensions d=128 (Method-2)}.} 
\end{figure}

\newpage
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/10.png}
\newline
\caption{ \textlatin{Classification Results with final dimensions d=256 (Method-2)}.} 
\end{figure}
\par
Απο την στιγμή που απο τα παραπάνω αποτελέσματα επιβεβαιώθηκε το γεγονός ότι μπορούμε να πάρουμε καλύτερο αποτέλεσμα ταξινόμησης εφαρμόζοντας την μέθοδο της διάσπασης του σετ εκπαίδευσης σε υποσύνολα, και μάλιστα σεπολύ μικρότερο χρόνο, εστιάσαμε την μελέτη της συγκεκριμένης μεθόδου στον τρόπο με τον οποίο γίνεται η επιλογή των δεδομένων με σκοπό την δημιουργία των τελικών υποσυνόλων. Σκεφτήκαμε λοιπόν την περίπτωση για την οποία θα μπορούσε να δημιουργηθεί ένας υποχώρος, ο οποίος να περιέχει την χρήσιμη πληροφορία απο ολόκληρο το σετ εκπαίδευσης. Δηλαδή, σύμφωνα με την παραπάνω μέθοδο απο όλα τα τελικά υποσύνολα. Ο τρόπος με τον οποίο προσπαθήσαμε να οδηγηθούμε σε αυτό το αποτέλεσμα είναι η εφαρμογή αλγορίθμων ομαδοποίησης των δεδομένων. Συγκεκριμένα εφαρμόστηκε ο αλγόριθμος \href{https://en.wikipedia.org/wiki/K-means_clustering}{\textlatin{K-means}} επιλέγοντας σαν τελικά αντιπροσωπευτικά σημεία για το τελικό σύνολο δεδομένων εκπαίδευσης, το αποτέλεσμα του αλγορίθμου το οποίο είναι τα κεντροειδή σημεία τα οποία αντιπροσωπεύουν το σύνολο των υπολοίπων. Ο κώδικας του συγκεκριμένου πειράματος βρίσκεται στον φάκελο \textlatin{LLE\textunderscore MNIST\textunderscore Kmeans}. Δοκιμάστηκαν διαφορετικές τιμές για το σύνολο των τελικών κεντροειδών, καταλήγοντας στο βέλτιστο αποτέλεσμα για την περίπτωση στην οποία έχουμε $K=9, d=128, clustSize=20.000$. Το σφάλμα της ταξινόμησης για την περίπτωση αυτή είναι 3.28\%, μικρότερο απο αυτό των αρχικών διαστάσεων (3.5\%). Συγκεκριμένες τιμές για τις παραμέτρους είναι $K=8,9,10,12,16,20$, \textlatin{d} όπως και στο προηγούμενο ερώτημα και $clustSize=5.000, 10.000, 15.000, 20.000$. Ενδιαφέρον παρουσιάζει το αποτέλεσμα της ταξινόμησης στον χώρο αρχικών διαστάσεων λαμβάνοντας υπόψιν ως δεδομένα εκπαίδευσης τα σημεία έπειτα απο την εφαρμογή του αλγορίθμου \textlatin{Kmeans} και όχι το σύνολο των δεδομένων εκπαίδευσης(60.000). Συγκεκριμένα έχουμε στον χώρο των αρχικών διαστάσεων με δεδομένα τα 5.000, 10.000, 15.000,20.0000 τελικά κεντροειδή ελάχιστα σφάλματα ταξινόμησης 4.43\%, 4.02\%, 3.67\%, 3.50\% μικρότερα απο το βέλτιστο σφάλμα μετά απο την μείωση των διαστάσεων. Η εξήγηση στο αποτέλεσμα αυτό, δίνεται απο το γεγονός ότι το σετ δεδομένων \textlatin{MNIST} είναι στην ουσία συνθετικό διότι ένας μικρός αριθμός του συνόλου των δεδομένων είναι μοναδικά και τα υπόλοιπα προκύπτουν απο ομογενείς μετασχηματισμούς ή παραμορφώσεις αυτών.
\par
Στο επόμενο πείραμα με το συγκεκριμένο σετ εφαρμόστηκε η μέθοδος-1, δηλαδή έγινε προβολή των δεδομένων αξιολόγησης στον χώρο μειωμένης διάστασης μέσω του πίνακα γειτνίασης στον χώρο των αρχικών διαστάσεων. Ο κώδικας του συγκεκριμένου πειράματος είναιστον φάκελο \textlatin{LLE\textunderscore MNIST\textunderscore PROJ}. Εκτελώντας τον κώδικα για την οπτικοποίηση των αποτελεσμάτων ταξινόμησης για κάθε συνδυασμό παραμέτρων βλέπουμε ότι το αποτέλεσμα της ταξινόμησης στον χώρο των τελικών διαστάσεων ισούτε με 3.85\%, μικρότερο απο αυτό των αρχικών διαστάσεων (3.5\%) οπότε θα μπορούσαμε να καταλήξουμε στο συμπέρασμα ότι η διαδικασία της μεθόδου δεν μας βοηθά στην βελτίωση του αποτελέσματος. Παρ' όλα αυτά αποτελεί έναν πολύ πρακτικό και γρήγορο τρόπο, σε σχέση με την εφαρμογή του αλγορίθμου στο σύνολο των δειγμάτων εκπαίδευσης και αξιολόγησης, ώστε να μειώσουμε τις διαστάσεις των δεδομένων αξιολόγησης. Αυτό είναι χρήσιμο σε περιπτώσεις στις οποίες μπορούμε νε ανεχθούμε την μικρή αυτή σχετικά διαφορά σφάλματος, και στις οποίες χρησιμοποιούμε τα δεδομένα αξιολόγησης σε πολλά επόμενα βήματα εκτελώντας υπολογιστικά απαιτητικούς αλγορίθμους. Το αποτέλεσμα εφαρμογής της μεθόδου για συγκεκριμένες τιμές των παραμέτρων φαίνεται στην εικόνα που ακολουθεί
\newpage
\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/11.png}
\newline
\caption{ \textlatin{Classification Results with final dimensions d=256 (Method-1)}.} 
\end{figure}

\par
\href{http://ufldl.stanford.edu/housenumbers/}{\textbf{\textlatin{SVHN: }}}Το δεύτερο σετ δεδομένων είναι το \href{http://ufldl.stanford.edu/housenumbers/}{\textlatin{The Street View House Numbers (SVHN) Dataset}} το οποίο περιέχει πραγματικές εικόνες απο αριθμούς σπιτιών, οι οποίες τραβήχτηκαν απο το αυτοκίνητο χαρτογράφησης της \textlatin{Google}. Συγκεκριμένα χρησιμοποιήθηκε το \textlatin{Format-2} στο οποίο οι αριθμοί με περισσότερα απο ένα ψηφία έχουν διασπαστεί σε ψηφία απο το 0 εως το 9. Το σετ αυτό, είναι όμοιο με το \textlatin{MNIST} με την διαφορά ότι πρόκειται για πραγματικές εικόνες, το οποίο καθιστά την διαδικασία της αναγνώρισης κατά πολύ δυσκολότερη. Αυτο συμβαίνει λόγω φυσικών παραμορφώσεων, για παράδειγμα απο την αλλοίωση της φωτεινότητας, την παρουσία θορύβου κλπ. Το σετ περιέχει 73.257 εικόνες στο σετ εκπαίδευσης και 26032 εικόνες στο σετ αξιολόγησης, μεγέθους $[32 \times 32]$ \textlatin{pixel}. Εφαρμόζοντας αντίστοιχα λεξικογραφική διάταξη στο σετ αυτό, καταλήγουμε να έχουμε για κάθε εικόνα ένα διάνυσμα μεγέθους $D = 1024$. 
\par
Αν για το συγκεκριμένο σετ δεδομένων εφαρμόσουμε την διαδικασία την οποία εφαρμόσαμε στο σετ δεδομένων \textlatin{MNIST} θα παρατηρήσουμε ότι τα αποτελέσματά μας είναι καταστροφικά. Αυτό συμβαίνει διότι, υπάρχει πολύ μεγάλο πρόβλημα απο το πρώτο κιόλας βήμα του αλγορίθμου στο οποίο εφαρμόζεται ο αλγόριθμος \textlatin{k-NN} μεταξύ των δεδομένων αξιολόγησης και των δεδομένων εκπαίδευσης στον χώρο των αρχικών διαστάσεων \textlatin{D}. Το γεγονός αυτό προκύπτει απο την φύση των δεδομένων του σετ αυτού, και πιο συγκεκριμένα ο αλγόριθμος κοντινότερων γειτόνων αποτυγχάνει εξαιτίας του γεγονότος ότι πρόκειται για εικόνες απο το φυσικό περιβάλλον οι οποίες πάσχουν απο θόρυβο αλλά και παραμορφώσεις της φωτεινότητας. 
\par
\href{https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients}{\textbf{\textlatin{Histogram of Oriented Gradients (HoG): }}} Για την αντιμετώπιση της παραπάνω αδυναμίας εκτέλεσης του αλγορίθμου, θα πρέπει να δώσουμε είσοδο τέτοια ώστε να είναι μοναδική για κάθε δείγμα του σετ δεδομένων ώστε να μπορεί ο αλγόριθμος να ομαδοποιήσει δεδομένα τα οποία ανήκουν στην ίδια κλάση, αλλά και να τα ξεχωρίσει απο αυτά των υπολοίπων. Η διαδικασία αυτή είναι γνωστή στον χώρο της υπολογιστικής όρασης και της επεξεργασίας εικόνας με τον όρο εξαγωγή χαρακτηριστικών. Όπως δηλώνει και το όνομά της, μέσω της διακασίας αυτής σε κάθε εικόνα εντοπίζονται συγκεκριμένα χαρακτηριστικά σημεία τα οποία την αντιπροσωπεύουν μοναδικά. Τα πιο συνηθισμένα τέτοιου είδους χαρακτηριστικά είναι τα \textlatin{SIFT} και τα \textlatin{HoG} τα οποία και εφαρμόστηκαν στην συγκεκριμένη περίπτωση λόγω καλύτερων αποτελεσμάτων σε αυτό το σετ δεδομένων. 
\par
Η βασική αρχή λειτουργίας της μεθόδου αυτής είναι ότι ένα συγκεκριμένο αντικείμενο μέσα σε μια εικόνα μπορεί να προσδιοριστεί απο την κατανομή της φωτεινότητας πάνω στα σημεία του αλλά και απο τον εντοπισμό της κλίσης της δηλαδή τον εντοπισμό των ακμών του. Ο αλγόριθμος χωρίζει την εικόνα σε μικρές περιοχές απο \textlatin{pixel} και σε κάθε μια απο αυτές υπολογίζει ένα ιστόγραμμα με την κλίση της φωτεινότητας. Το μέγεθος των περιοχών αυτών καθορίζεται απο το μέγεθος του πυρήνα ο οποίος και ορίζεται κατά την εκτέλεση του αλγορίθμου.

\subsubsection{Πειράματα - \textlatin{SVHN}}
\par
Κατά το πρώτο πείραμα λοιπόν με αυτό το σετ δεδομένων μελετήθηκε τόσο η συμπεριφορά του αλγορίθμου \textlatin{LLE} αλλάζοντας τις παραμέτρους του όσο και η αποτελεσματικότητα των \textlatin{HoG} χαρακτηριστικών ανάλογα με το μέγεθος του πυρήνα. Πιο συγκεκριμένα για τις παραμέτρους του αλγορίθμου \textlatin{LLE} δώθηκαν οι τιμές $Κ=8,10,12$ και $d=16,20,32,64,96,128,164,196,256$ και για το μέγεθος του πυρήνα του αλγορίθμου \textlatin{HoG} οι τιμές $kernels=[2\times2],[4\times4],[8\times8]$. Επίσης χρησιμοποιήθηκαν απο το σετ εκπαίδευσης 30.000 δείγματα, δηλαδή 3.000 εικόνες ψηφίων απο κάθε κλάση.
